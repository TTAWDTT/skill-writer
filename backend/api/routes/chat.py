"""
Chat API è·¯ç”±
å¤„ç†ä¸å·¥ä½œæµçš„äº¤äº’å¯¹è¯ï¼Œæ”¯æŒæµå¼è¾“å‡º
"""
from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional, List, Any, Dict
from pathlib import Path
import asyncio
import re
import json
import logging
import time
import datetime
import base64
import uuid
import os

import httpx

from backend.core.workflow import get_workflow
from backend.core.skills.registry import get_registry
from backend.core.agents.file_extractor import (
    parse_uploaded_file,
    extract_info_from_multiple_files,
    generate_field_from_files,
)
from backend.core.agents.skill_fixer_agent import SkillFixerAgent
from backend.core.llm.config_store import has_llm_credentials, get_llm_config

try:
    import multipart  # noqa: F401
    MULTIPART_AVAILABLE = True
except Exception:
    MULTIPART_AVAILABLE = False

router = APIRouter()

logger = logging.getLogger(__name__)

_UPLOAD_PARSE_CONCURRENCY = max(1, min(int(os.getenv("UPLOAD_PARSE_CONCURRENCY", "4")), 12))
_UPLOAD_SESSION_LOCKS: Dict[str, asyncio.Lock] = {}
_UPLOAD_SESSION_LOCKS_GUARD = asyncio.Lock()


def _redact_secrets(message: str) -> str:
    if not message:
        return message
    # Redact common token formats (best-effort)
    patterns = [
        (re.compile(r"(sk-[A-Za-z0-9]{8,})"), "sk-***"),
        (re.compile(r"(gho_[A-Za-z0-9]{8,})"), "gho_***"),
        (re.compile(r"(Bearer\\s+)[A-Za-z0-9._\\-]{10,}"), r"\\1***"),
        (re.compile(r"(api[-_ ]?key\\s*[:=]\\s*)([^\\s,;]+)", re.IGNORECASE), r"\\1***"),
        (re.compile(r"(\*{2,}[A-Za-z0-9]{2,})"), "***"),
    ]
    sanitized = message
    for pattern, replacement in patterns:
        sanitized = pattern.sub(replacement, sanitized)
    return sanitized


def _ensure_llm_configured():
    if not has_llm_credentials():
        raise HTTPException(status_code=400, detail="æ¨¡å‹æœªé…ç½®")


def _resolve_skill_system_prompt(skill: Any) -> str:
    """Best-effort extraction of system prompt from wrapped skills."""
    if skill is None:
        return ""

    queue: List[Any] = [skill]
    seen: set[int] = set()
    fallback_guidelines = ""

    while queue:
        cur = queue.pop(0)
        if cur is None:
            continue
        cur_id = id(cur)
        if cur_id in seen:
            continue
        seen.add(cur_id)

        for attr in ("system_prompt", "_system_prompt"):
            try:
                value = getattr(cur, attr, "")
            except Exception:
                value = ""
            if isinstance(value, str) and value.strip():
                return value.strip()

        try:
            guidelines = getattr(cur, "writing_guidelines", "")
        except Exception:
            guidelines = ""
        if isinstance(guidelines, str) and guidelines.strip() and not fallback_guidelines:
            fallback_guidelines = guidelines.strip()

        for attr in ("_concrete", "_base", "base_skill", "wrapped_skill", "_wrapped", "inner_skill"):
            try:
                nested = getattr(cur, attr, None)
            except Exception:
                nested = None
            if nested is not None:
                queue.append(nested)

    return fallback_guidelines


class StartSessionRequest(BaseModel):
    """å¼€å§‹ä¼šè¯è¯·æ±‚"""
    skill_id: str


class ChatRequest(BaseModel):
    """å¯¹è¯è¯·æ±‚"""
    session_id: str
    message: str


class SessionResponse(BaseModel):
    """ä¼šè¯å“åº”"""
    session_id: str
    phase: str
    message: str
    is_complete: bool
    document: Optional[str] = None


class UploadFilePayload(BaseModel):
    """JSON ä¸Šä¼ æ–‡ä»¶"""
    filename: str
    content_base64: str
    content_type: Optional[str] = None


class UploadFilesRequest(BaseModel):
    """JSON ä¸Šä¼ è¯·æ±‚"""
    files: List[UploadFilePayload]


class GenerateFieldRequest(BaseModel):
    """ç”Ÿæˆå•ä¸ªå­—æ®µè¯·æ±‚"""
    field_id: str


class WebSearchRequest(BaseModel):
    """Web æœç´¢è¯·æ±‚"""
    query: str
    top_k: int = 5


class DiagramRequest(BaseModel):
    """ç”Ÿæˆå›¾ç¤ºè¯·æ±‚"""
    title: Optional[str] = None
    diagram_type: str = "technical_route"  # technical_route | research_framework | freestyle | infographic
    mode: str = "infographic"  # infographic | image_model | auto
    # é€‰åŒºç”Ÿæˆï¼šselected_text ä¸ºæ ¸å¿ƒï¼Œcontext_text ä¸ºå…¨æ–‡ä¸Šä¸‹æ–‡
    selected_text: Optional[str] = None
    context_text: Optional[str] = None


class GenerateIllustrationsRequest(BaseModel):
    """è‡ªåŠ¨ç”Ÿæˆé…å›¾è¯·æ±‚ï¼ˆè¾“å…¥ä¸ºæ•´ç¯‡æ–‡ç« ï¼‰"""
    document_content: str
    mode: str = "infographic"  # infographic | image_model | auto
    max_images: int = 2


@router.post("/start", response_model=SessionResponse)
async def start_session(request: StartSessionRequest):
    """
    å¼€å§‹æ–°ä¼šè¯

    - ä¼ å…¥ skill_idï¼Œåˆ›å»ºæ–°ä¼šè¯
    - è¿”å›åˆå§‹é—®å€™è¯­å’Œ session_id
    """
    # éªŒè¯ skill å­˜åœ¨
    registry = get_registry()
    skill = registry.get(request.skill_id)
    if not skill:
        raise HTTPException(status_code=404, detail=f"Skill not found: {request.skill_id}")

    _ensure_llm_configured()

    # å¼€å§‹ä¼šè¯
    workflow = get_workflow()
    result = await workflow.start_session(request.skill_id)

    if "error" in result:
        raise HTTPException(status_code=500, detail=result["error"])

    return SessionResponse(
        session_id=result["session_id"],
        phase=result["phase"],
        message=result["message"],
        is_complete=result["is_complete"],
    )


@router.post("/message", response_model=SessionResponse)
async def send_message(request: ChatRequest):
    """
    å‘é€æ¶ˆæ¯

    - åœ¨éœ€æ±‚æ”¶é›†é˜¶æ®µï¼Œå‘é€ç”¨æˆ·å›å¤
    - å¦‚æœéœ€æ±‚æ”¶é›†å®Œæˆï¼Œè‡ªåŠ¨è¿›å…¥å†™ä½œé˜¶æ®µ
    """
    _ensure_llm_configured()
    workflow = get_workflow()
    result = await workflow.chat(request.session_id, request.message)

    if "error" in result:
        raise HTTPException(status_code=400, detail=result["error"])

    return SessionResponse(
        session_id=result["session_id"],
        phase=result["phase"],
        message=result["message"],
        is_complete=result.get("is_complete", False),
        document=result.get("document"),
    )


@router.post("/generate/{session_id}")
async def generate_document(session_id: str):
    """
    ç”Ÿæˆæ–‡æ¡£ï¼ˆéæµå¼ï¼‰

    - åœ¨ writing é˜¶æ®µè°ƒç”¨
    - è¿”å›ç”Ÿæˆçš„å®Œæ•´æ–‡æ¡£
    """
    _ensure_llm_configured()
    workflow = get_workflow()
    result = await workflow.generate_document(session_id)

    if "error" in result:
        raise HTTPException(status_code=400, detail=result["error"])

    return SessionResponse(
        session_id=result["session_id"],
        phase=result["phase"],
        message=result["message"],
        is_complete=result.get("is_complete", False),
        document=result.get("document"),
    )


@router.get("/generate/{session_id}/stream")
async def generate_document_stream(session_id: str):
    """
    æµå¼ç”Ÿæˆæ–‡æ¡£ï¼ˆSSEï¼‰

    - åœ¨ writing é˜¶æ®µè°ƒç”¨
    - å®æ—¶è¿”å›ç”Ÿæˆè¿‡ç¨‹
    """
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    if session.phase != "writing":
        raise HTTPException(
            status_code=400,
            detail=f"Session not in writing phase: {session.phase}"
        )

    _ensure_llm_configured()

    async def event_generator():
        try:
            async for event in workflow.generate_document_stream(session_id):
                # æ ¼å¼åŒ–ä¸º SSE
                data = json.dumps(event, ensure_ascii=False)
                yield f"data: {data}\n\n"
        except Exception as e:
            error_event = json.dumps({"type": "error", "error": str(e)})
            yield f"data: {error_event}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


@router.get("/session/{session_id}")
async def get_session(session_id: str):
    """è·å–ä¼šè¯çŠ¶æ€"""
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    return {
        "session_id": session.session_id,
        "skill_id": session.skill_id,
        "phase": session.phase,
        "has_document": session.final_document is not None,
        "created_at": session.created_at,
        "updated_at": session.updated_at,
        "message_count": len(session.messages),
    }


@router.get("/session/{session_id}/messages")
async def get_session_messages(session_id: str):
    """è·å–ä¼šè¯æ¶ˆæ¯å†å²"""
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    return {
        "session_id": session.session_id,
        "messages": session.messages,
    }


@router.get("/session/{session_id}/document")
async def get_session_document(session_id: str):
    """è·å–ä¼šè¯ç”Ÿæˆçš„æ–‡æ¡£"""
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    if not session.final_document:
        raise HTTPException(status_code=404, detail="Document not generated yet")

    return {
        "session_id": session.session_id,
        "document": session.final_document,
        "sections": session.sections,
    }

def _build_skill_fields(skill) -> List[dict]:
    target_fields = list(skill.requirement_fields)

    collection_rank = {"required": 0, "infer": 1, "optional": 2}
    target_fields.sort(
        key=lambda f: (
            collection_rank.get(f.collection, 2),
            f.priority,
            f.name,
        )
    )
    return [
        {
            "id": f.id,
            "name": f.name,
            "description": f.description,
            "type": f.field_type,
            "required": f.required,
            "collection": f.collection,
            "priority": f.priority,
            "example": f.example,
        }
        for f in target_fields
    ]


def _try_parse_json_value(value: Any):
    if not isinstance(value, str):
        return value
    stripped = value.strip()
    if not stripped:
        return value

    candidates = []
    if (stripped.startswith("{") and stripped.endswith("}")) or (stripped.startswith("[") and stripped.endswith("]")):
        candidates.append(stripped)

    fence_matches = re.findall(r"```(?:json)?\s*([\s\S]*?)\s*```", stripped)
    candidates.extend(fence_matches)

    for pattern in (r"\{[\s\S]*?\}", r"\[[\s\S]*?\]"):
        match = re.search(pattern, stripped)
        if match:
            candidates.append(match.group())

    for candidate in candidates:
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            continue

    return value


def _flatten_list_value(values: list, field_type: str, field_id: str) -> str:
    separator = "\n" if field_type == "textarea" else "ã€"
    parts = []
    for item in values:
        normalized = _normalize_extracted_value(item, field_type, "", field_id)
        if normalized is None:
            continue
        parts.append(str(normalized))
    return separator.join(parts).strip()


def _normalize_key(text: str) -> str:
    return re.sub(r"[^0-9a-zA-Z\u4e00-\u9fff]+", "", str(text)).lower()


def _find_partial_key(tokens: list, key_lookup_normalized: dict) -> Optional[str]:
    for token in tokens:
        normalized_token = _normalize_key(token)
        if not normalized_token:
            continue
        for normalized_key, original in key_lookup_normalized.items():
            if normalized_token in normalized_key:
                return original
    return None


def _extract_value_from_unparsed_json(text: str, keys: list) -> Optional[str]:
    if not isinstance(text, str):
        return None
    if not keys:
        return None

    for key in keys:
        if not key:
            continue
        escaped = re.escape(str(key))
        patterns = [
            rf'"{escaped}"\s*:\s*"([\s\S]*?)"',
            rf"'{escaped}'\s*:\s*'([\s\S]*?)'",
            rf'â€œ{escaped}â€\s*:\s*â€œ([\s\S]*?)â€',
        ]
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()

    return None


def _flatten_dict_value(value_map: dict, field_type: str, field_name: str, field_id: str) -> str:
    if not value_map:
        return ""

    name_hint = (field_name or "").lower()
    prefer_title = field_type != "textarea" and any(k in name_hint for k in ["name", "title", "åç§°", "æ ‡é¢˜", "é¢˜ç›®"])
    content_keys = ["content", "æ­£æ–‡", "å†…å®¹", "text", "body", "detail", "details", "description", "summary", "ç®€ä»‹", "è¯´æ˜", "èƒŒæ™¯"]
    title_keys = ["title", "æ ‡é¢˜", "name", "åç§°", "topic", "subject", "é¡¹ç›®åç§°", "è¯¾é¢˜åç§°"]

    key_order = title_keys + content_keys if prefer_title else content_keys + title_keys
    key_lookup = {str(k).lower(): k for k in value_map.keys()}
    key_lookup_normalized = {_normalize_key(k): k for k in value_map.keys()}
    field_id_key = _normalize_key(field_id)
    field_name_key = _normalize_key(field_name)

    if field_id_key in key_lookup_normalized:
        raw_value = value_map.get(key_lookup_normalized[field_id_key])
        normalized = _normalize_extracted_value(raw_value, field_type, field_name, field_id)
        return str(normalized).strip() if normalized is not None else ""

    if field_name_key and field_name_key in key_lookup_normalized:
        raw_value = value_map.get(key_lookup_normalized[field_name_key])
        normalized = _normalize_extracted_value(raw_value, field_type, field_name, field_id)
        return str(normalized).strip() if normalized is not None else ""

    partial_key = _find_partial_key([field_id, field_name], key_lookup_normalized)
    if partial_key:
        raw_value = value_map.get(partial_key)
        normalized = _normalize_extracted_value(raw_value, field_type, field_name, field_id)
        return str(normalized).strip() if normalized is not None else ""

    for key in key_order:
        if key in key_lookup:
            raw_value = value_map.get(key_lookup[key])
            normalized = _normalize_extracted_value(raw_value, field_type, field_name, field_id)
            if normalized is None:
                continue
            return str(normalized).strip()

    partial_key = _find_partial_key(key_order, key_lookup_normalized)
    if partial_key:
        raw_value = value_map.get(partial_key)
        normalized = _normalize_extracted_value(raw_value, field_type, field_name, field_id)
        return str(normalized).strip() if normalized is not None else ""

    if len(value_map) == 1:
        only_value = next(iter(value_map.values()))
        normalized = _normalize_extracted_value(only_value, field_type, field_name, field_id)
        return str(normalized).strip() if normalized is not None else ""

    separator = "\n" if field_type == "textarea" else "ï¼Œ"
    parts = []
    for key, raw_value in value_map.items():
        normalized = _normalize_extracted_value(raw_value, field_type, field_name, field_id)
        if normalized is None:
            continue
        parts.append(f"{key}: {normalized}")
    return separator.join(parts).strip()


def _normalize_extracted_value(value: Any, field_type: str, field_name: str, field_id: str) -> Any:
    if value is None:
        return None

    parsed = _try_parse_json_value(value)

    if isinstance(parsed, dict):
        return _flatten_dict_value(parsed, field_type, field_name, field_id)
    if isinstance(parsed, list):
        return _flatten_list_value(parsed, field_type, field_id)
    if isinstance(parsed, str):
        stripped = parsed.strip()
        if stripped.startswith("{") or stripped.startswith("["):
            title_keys = ["title", "name", "æ ‡é¢˜", "åç§°", "topic", "subject", "é¡¹ç›®åç§°", "è¯¾é¢˜åç§°", "project_title"]
            content_keys = ["content", "æ­£æ–‡", "å†…å®¹", "text", "body", "detail", "details", "description", "summary", "ç®€ä»‹", "è¯´æ˜", "èƒŒæ™¯"]
            key_candidates = [field_id, field_name] + title_keys + content_keys
            extracted = _extract_value_from_unparsed_json(stripped, key_candidates)
            if extracted:
                return extracted

    return parsed


def _normalize_extracted_fields(extracted_fields: Dict[str, Any], skill) -> Dict[str, Any]:
    if not extracted_fields:
        return {}

    field_map = {f.id: f for f in skill.requirement_fields}
    normalized = {}
    for field_id, value in extracted_fields.items():
        field = field_map.get(field_id)
        field_type = field.field_type if field else "text"
        field_name = field.name if field else field_id
        normalized_value = _normalize_extracted_value(value, field_type, field_name, field_id)
        if normalized_value is None:
            continue
        if isinstance(normalized_value, str) and not normalized_value.strip():
            continue
        normalized[field_id] = normalized_value
    return normalized


def _trim_file_content(content: str, max_chars: int = 20000) -> str:
    """é™åˆ¶å­˜å‚¨çš„æ–‡ä»¶å†…å®¹é•¿åº¦ï¼Œé¿å…æ•°æ®åº“è¿‡å¤§"""
    if not content:
        return ""
    content = content.strip()
    if len(content) <= max_chars:
        return content
    return content[:max_chars]


async def _get_upload_session_lock(session_id: str) -> asyncio.Lock:
    """Per-session lock for merge/save to avoid concurrent upload write conflicts."""
    async with _UPLOAD_SESSION_LOCKS_GUARD:
        lock = _UPLOAD_SESSION_LOCKS.get(session_id)
        if lock is None:
            lock = asyncio.Lock()
            _UPLOAD_SESSION_LOCKS[session_id] = lock
        return lock


async def _parse_json_upload_file(
    idx: int,
    file: UploadFilePayload,
    allowed_extensions: set[str],
) -> tuple[int, Optional[dict], str]:
    file_ext = Path(file.filename).suffix.lower()
    if file_ext not in allowed_extensions:
        return idx, None, f"âŒ {file.filename}: ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ ({file_ext})"

    try:
        content = base64.b64decode(file.content_base64)
        text_content = await asyncio.to_thread(parse_uploaded_file, content, file_ext, file.filename)
        if text_content:
            parsed = {
                "filename": file.filename,
                "content": text_content,
                "content_type": file.content_type or "",
                "size": len(content),
            }
            return idx, parsed, f"âœ… {file.filename}: è§£ææˆåŠŸ ({len(text_content)} å­—ç¬¦)"
        return idx, None, f"âš ï¸ {file.filename}: æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è§£æ"
    except Exception as e:
        return idx, None, f"âŒ {file.filename}: è§£æå¤±è´¥ - {str(e)}"


async def _parse_multipart_upload_file(
    idx: int,
    file: UploadFile,
    allowed_extensions: set[str],
) -> tuple[int, Optional[dict], str]:
    filename = file.filename or f"uploaded_{idx}"
    file_ext = Path(filename).suffix.lower()
    if file_ext not in allowed_extensions:
        return idx, None, f"âŒ {filename}: ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ ({file_ext})"

    try:
        content = await file.read()
        text_content = await asyncio.to_thread(parse_uploaded_file, content, file_ext, filename)
        if text_content:
            parsed = {
                "filename": filename,
                "content": text_content,
                "content_type": file.content_type or "",
                "size": len(content),
            }
            return idx, parsed, f"âœ… {filename}: è§£ææˆåŠŸ ({len(text_content)} å­—ç¬¦)"
        return idx, None, f"âš ï¸ {filename}: æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è§£æ"
    except Exception as e:
        return idx, None, f"âŒ {filename}: è§£æå¤±è´¥ - {str(e)}"


async def _parse_json_files_parallel(files: List[UploadFilePayload], allowed_extensions: set[str]) -> tuple[List[dict], List[str]]:
    sem = asyncio.Semaphore(_UPLOAD_PARSE_CONCURRENCY)

    async def _worker(idx: int, file: UploadFilePayload):
        async with sem:
            return await _parse_json_upload_file(idx, file, allowed_extensions)

    tasks = [asyncio.create_task(_worker(i, f)) for i, f in enumerate(files)]
    results = await asyncio.gather(*tasks)
    results.sort(key=lambda item: item[0])

    parsed_files: List[dict] = []
    file_summaries: List[str] = []
    for _, parsed, summary in results:
        file_summaries.append(summary)
        if parsed:
            parsed_files.append(parsed)
    return parsed_files, file_summaries


async def _parse_multipart_files_parallel(files: List[UploadFile], allowed_extensions: set[str]) -> tuple[List[dict], List[str]]:
    sem = asyncio.Semaphore(_UPLOAD_PARSE_CONCURRENCY)

    async def _worker(idx: int, file: UploadFile):
        async with sem:
            return await _parse_multipart_upload_file(idx, file, allowed_extensions)

    tasks = [asyncio.create_task(_worker(i, f)) for i, f in enumerate(files)]
    results = await asyncio.gather(*tasks)
    results.sort(key=lambda item: item[0])

    parsed_files: List[dict] = []
    file_summaries: List[str] = []
    for _, parsed, summary in results:
        file_summaries.append(summary)
        if parsed:
            parsed_files.append(parsed)
    return parsed_files, file_summaries


async def _handle_parsed_upload(
    session,
    skill,
    workflow,
    parsed_files: List[dict],
    file_summaries: List[str],
):
    # ä¸Šä¼ ææ–™åº”å°½é‡â€œå¯ç”¨ä¼˜å…ˆâ€ï¼šå³ä½¿æ¨¡å‹æœªé…ç½®/æš‚æ—¶ä¸å¯ç”¨ï¼Œä¹Ÿå…ˆä¿å­˜è§£æå‡ºçš„æ–‡æœ¬ï¼Œ
    # åªæ˜¯è·³è¿‡è‡ªåŠ¨ä¿¡æ¯æå–ä¸ Skill Fixerï¼ˆä¾èµ– LLMï¼‰ã€‚
    if not parsed_files:
        return {
            "success": False,
            "session_id": session.session_id,
            "message": "æ²¡æœ‰æˆåŠŸè§£æä»»ä½•æ–‡ä»¶",
            "file_results": file_summaries,
            "extracted_fields": {},
            "external_information": "",
        }

    skill_fields = _build_skill_fields(skill)
    extraction_result = {
        "extracted_fields": {},
        "external_information": "",
        "summaries": "",
    }
    warning: Optional[str] = None
    extraction_ms: Optional[int] = None
    llm_used: Optional[dict] = None

    if has_llm_credentials():
        try:
            cfg = get_llm_config()
            llm_used = {
                "provider_name": getattr(cfg, "provider_name", ""),
                "provider": getattr(cfg, "provider", ""),
                "base_url": getattr(cfg, "base_url", ""),
                "model": getattr(cfg, "model", ""),
            }
            logger.info(
                "[upload] start llm extraction session=%s files=%s provider=%s model=%s base_url=%s",
                session.session_id,
                len(parsed_files),
                getattr(cfg, "provider_name", ""),
                getattr(cfg, "model", ""),
                getattr(cfg, "base_url", ""),
            )
            start_ts = time.time()
            extraction_result = await extract_info_from_multiple_files(
                files=parsed_files,
                skill_fields=skill_fields,
                skill_name=skill.metadata.name,
                existing_requirements=session.requirements,
            )
            extraction_ms = int((time.time() - start_ts) * 1000)
        except Exception as e:
            # ä¸é˜»æ–­ä¸Šä¼ ï¼šå…è®¸ç”¨æˆ·ç»§ç»­ç”Ÿæˆ/æ‰‹å·¥è¡¥å……ï¼Œåªæ˜¯å¤±å»è‡ªåŠ¨æå–èƒ½åŠ›
            warning = f"æ–‡ä»¶å·²è§£æå¹¶ä¿å­˜ï¼Œä½†è‡ªåŠ¨ä¿¡æ¯æå–å¤±è´¥ï¼š{_redact_secrets(str(e))}"
            extraction_result = {
                "extracted_fields": {},
                "external_information": "",
                "summaries": "",
            }
    else:
        warning = "æ¨¡å‹æœªé…ç½®ï¼šæ–‡ä»¶å·²è§£æå¹¶ä¿å­˜ï¼Œä½†ä¸ä¼šè‡ªåŠ¨æå–ä¿¡æ¯ã€‚"

    extraction_result["extracted_fields"] = _normalize_extracted_fields(
        extraction_result.get("extracted_fields", {}),
        skill,
    )

    external_info = extraction_result.get("external_information", "")
    extracted_fields = extraction_result.get("extracted_fields", {})
    upload_message = f"ğŸ“ å·²ä¸Šä¼  {len(parsed_files)} ä¸ªæ–‡ä»¶å¹¶æå–ä¿¡æ¯ï¼š\n" + "\n".join(file_summaries)
    merged_external_information = session.external_information or ""

    # Merge/save must be serialized per session; uploads may arrive concurrently.
    session_lock = await _get_upload_session_lock(session.session_id)
    async with session_lock:
        latest_session = workflow.get_session(session.session_id)
        if not latest_session:
            raise HTTPException(status_code=404, detail=f"Session not found: {session.session_id}")

        for pf in parsed_files:
            latest_session.add_uploaded_file({
                "filename": pf["filename"],
                "content_type": pf.get("content_type", ""),
                "size": pf.get("size", 0),
                "content": _trim_file_content(pf.get("content", "")),
                "extracted_fields": extracted_fields,
            })

        if external_info:
            latest_session.append_external_info(external_info)

        if extracted_fields:
            if latest_session.requirements is None:
                latest_session.requirements = {}
            for field_id, value in extracted_fields.items():
                if value is None:
                    continue
                if isinstance(value, str) and not value.strip():
                    continue
                existing_value = latest_session.requirements.get(field_id)
                if existing_value is None or (isinstance(existing_value, str) and not existing_value.strip()):
                    latest_session.requirements[field_id] = value

        latest_session.messages.append({
            "role": "system",
            "content": upload_message,
        })
        workflow.save_session(latest_session)
        merged_external_information = latest_session.external_information or ""

    # Skill-Fixer è°ƒç”¨è¾ƒé‡ï¼Œä¸æŒé”æ‰§è¡Œï¼›æ‰§è¡Œåå†çŸ­æš‚æŒé”è½åº“ã€‚
    if warning is None:
        try:
            fixer = SkillFixerAgent()
            fixer_result = await fixer.run(
                skill=skill,
                extracted_fields=extracted_fields,
                external_information=merged_external_information,
                file_summaries=extraction_result.get("summaries", ""),
            )
            overlay = {
                "writing_guidelines_additions": fixer_result.writing_guidelines_additions,
                "global_principles": fixer_result.global_principles,
                "section_overrides": fixer_result.section_overrides,
                "relax_requirements": fixer_result.relax_requirements,
                "material_context": fixer_result.material_context,
                "section_prompt_overrides": fixer_result.section_prompt_overrides,
            }
            async with session_lock:
                latest_session = workflow.get_session(session.session_id)
                if latest_session:
                    latest_session.skill_overlay = overlay
                    workflow.save_session(latest_session)
        except Exception as e:
            print(f"[Skill Fixer Warning] {e}")

    return {
        "success": True,
        "session_id": session.session_id,
        "message": f"æˆåŠŸå¤„ç† {len(parsed_files)} ä¸ªæ–‡ä»¶",
        "warning": warning,
        "llm_used": llm_used,
        "extraction_ms": extraction_ms,
        "file_results": file_summaries,
        "extracted_fields": extracted_fields,
        "external_information": external_info[:500] + "..." if len(external_info) > 500 else external_info,
        "summaries": extraction_result.get("summaries", ""),
    }


@router.post("/session/{session_id}/upload-json")
async def upload_files_json(
    session_id: str,
    payload: UploadFilesRequest,
):
    """
    ä¸Šä¼ æ–‡ä»¶åˆ°ä¼šè¯

    - æ”¯æŒä¸Šä¼ å¤šä¸ªæ–‡ä»¶
    - è‡ªåŠ¨è§£ææ–‡ä»¶å†…å®¹å¹¶ä½¿ç”¨ LLM æå–ç›¸å…³ä¿¡æ¯
    - è¿”å›æå–çš„ä¿¡æ¯æ‘˜è¦
    """
    # éªŒè¯ä¼šè¯å­˜åœ¨
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    if session.phase not in ["init", "requirement"]:
        raise HTTPException(
            status_code=400,
            detail=f"Cannot upload files in phase: {session.phase}. Only allowed during requirement collection."
        )

    # è·å– Skill ä¿¡æ¯
    registry = get_registry()
    skill = registry.get(session.skill_id)

    if not skill:
        raise HTTPException(status_code=404, detail=f"Skill not found: {session.skill_id}")

    # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
    allowed_extensions = {'.md', '.txt', '.doc', '.docx', '.pdf', '.pptx'}

    parsed_files, file_summaries = await _parse_json_files_parallel(payload.files, allowed_extensions)

    try:
        return await _handle_parsed_upload(session, skill, workflow, parsed_files, file_summaries)
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        print(f"[File Upload Error] {traceback.format_exc()}")
        raise HTTPException(
            status_code=500,
            detail=str(e)
        )


if MULTIPART_AVAILABLE:
    @router.post("/session/{session_id}/upload")
    async def upload_files(
        session_id: str,
        files: List[UploadFile] = File(...),
    ):
        """
        ä¸Šä¼ æ–‡ä»¶åˆ°ä¼šè¯

        - æ”¯æŒä¸Šä¼ å¤šä¸ªæ–‡ä»¶
        - è‡ªåŠ¨è§£ææ–‡ä»¶å†…å®¹å¹¶ä½¿ç”¨ LLM æå–ç›¸å…³ä¿¡æ¯
        - è¿”å›æå–çš„ä¿¡æ¯æ‘˜è¦
        """
        # éªŒè¯ä¼šè¯å­˜åœ¨
        workflow = get_workflow()
        session = workflow.get_session(session_id)

        if not session:
            raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

        if session.phase not in ["init", "requirement"]:
            raise HTTPException(
                status_code=400,
                detail=f"Cannot upload files in phase: {session.phase}. Only allowed during requirement collection."
            )

        # è·å– Skill ä¿¡æ¯
        registry = get_registry()
        skill = registry.get(session.skill_id)

        if not skill:
            raise HTTPException(status_code=404, detail=f"Skill not found: {session.skill_id}")

        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        allowed_extensions = {'.md', '.txt', '.doc', '.docx', '.pdf', '.pptx'}

        parsed_files, file_summaries = await _parse_multipart_files_parallel(files, allowed_extensions)

        try:
            return await _handle_parsed_upload(session, skill, workflow, parsed_files, file_summaries)
        except HTTPException:
            raise
        except Exception as e:
            import traceback
            print(f"[File Upload Error] {traceback.format_exc()}")
            raise HTTPException(
                status_code=500,
                detail=str(e)
            )


@router.get("/session/{session_id}/files")
async def get_session_files(session_id: str):
    """è·å–ä¼šè¯ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨"""
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    files = [
        {k: v for k, v in file_info.items() if k != "content"}
        for file_info in session.uploaded_files
    ]

    return {
        "session_id": session.session_id,
        "files": files,
        "external_information": session.external_information,
    }


@router.post("/session/{session_id}/generate-field")
async def generate_field(session_id: str, request: GenerateFieldRequest):
    """åŸºäºå·²ä¸Šä¼ ææ–™ç”Ÿæˆå•ä¸ªå­—æ®µå†…å®¹"""
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    if not session.uploaded_files:
        raise HTTPException(status_code=400, detail="No uploaded files found for this session")

    if not has_llm_credentials():
        raise HTTPException(status_code=400, detail="æ¨¡å‹æœªé…ç½®")

    registry = get_registry()
    skill = registry.get(session.skill_id)
    if not skill:
        raise HTTPException(status_code=404, detail=f"Skill not found: {session.skill_id}")

    field = next((f for f in skill.requirement_fields if f.id == request.field_id), None)
    if not field:
        raise HTTPException(status_code=404, detail=f"Field not found: {request.field_id}")

    files = [
        {
            "filename": f.get("filename", "unknown"),
            "content": f.get("content", ""),
        }
        for f in session.uploaded_files
    ]

    if not any(f.get("content") for f in files):
        raise HTTPException(status_code=400, detail="No file content available; please re-upload files")

    result = await generate_field_from_files(
        files=files,
        field={
            "id": field.id,
            "name": field.name,
            "description": field.description,
            "type": field.field_type,
        },
        skill_name=skill.metadata.name,
        existing_requirements=session.requirements,
        external_information=session.external_information,
    )

    value = result.get("value")
    value = _normalize_extracted_value(value, field.field_type, field.name, field.id)
    if value is None or (isinstance(value, str) and not value.strip()):
        return {
            "success": False,
            "session_id": session_id,
            "field_id": field.id,
            "message": "æœªåœ¨ææ–™ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯",
            "value": None,
        }

    if session.requirements is None:
        session.requirements = {}
    session.requirements[field.id] = value
    workflow.save_session(session)

    return {
        "success": True,
        "session_id": session_id,
        "field_id": field.id,
        "value": value,
    }


def _format_search_sources(query: str, results: List[dict]) -> str:
    ts = datetime.datetime.now().isoformat(timespec="seconds")
    lines = [f"## Web Searchï¼ˆ{ts}ï¼‰", f"æŸ¥è¯¢ï¼š{query}", ""]
    for idx, r in enumerate(results, start=1):
        title = (r.get("title") or "").strip()
        url = (r.get("url") or "").strip()
        snippet = (r.get("snippet") or "").strip()
        lines.append(f"{idx}. {title}")
        if url:
            lines.append(f"   - URL: {url}")
        if snippet:
            lines.append(f"   - æ‘˜è¦: {snippet}")
    return "\n".join(lines).strip()


async def _duckduckgo_search(query: str, top_k: int = 5) -> List[dict]:
    q = (query or "").strip()
    if not q:
        return []
    top_k = max(1, min(int(top_k or 5), 10))

    url = "https://duckduckgo.com/html/"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122 Safari/537.36",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.7",
    }
    async with httpx.AsyncClient(headers=headers, timeout=30.0, follow_redirects=True) as client:
        resp = await client.post(url, data={"q": q})
        resp.raise_for_status()
        html = resp.text

    results: List[dict] = []
    for m in re.finditer(r'<a[^>]+class="result__a"[^>]+href="([^"]+)"[^>]*>([\s\S]*?)</a>', html):
        href = m.group(1)
        raw_title = m.group(2)
        title = re.sub(r"<[^>]+>", "", raw_title)
        title = re.sub(r"\s+", " ", title).strip()

        snippet = ""
        tail = html[m.end(): m.end() + 2500]
        sm = re.search(r'class="result__snippet"[^>]*>([\s\S]*?)</a>|class="result__snippet"[^>]*>([\s\S]*?)</div>', tail)
        if sm:
            raw = sm.group(1) or sm.group(2) or ""
            snippet = re.sub(r"<[^>]+>", "", raw)
            snippet = re.sub(r"\s+", " ", snippet).strip()

        if href and title:
            results.append({"title": title, "url": href, "snippet": snippet})
        if len(results) >= top_k:
            break
    return results


@router.post("/session/{session_id}/search-web")
async def search_web(session_id: str, request: WebSearchRequest):
    """Web æœç´¢å¹¶æŠŠæ¥æºè¿½åŠ åˆ°ä¼šè¯ external_informationï¼ˆç”¨äºåç»­å†™ä½œå¼•ç”¨/èƒŒæ™¯è¡¥å……ï¼‰"""
    workflow = get_workflow()
    session = workflow.get_session(session_id)
    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    query = (request.query or "").strip()
    if not query:
        raise HTTPException(status_code=400, detail="query ä¸èƒ½ä¸ºç©º")

    results = await _duckduckgo_search(query, request.top_k)
    if not results:
        return {"success": True, "session_id": session_id, "query": query, "results": [], "message": "æœªæ£€ç´¢åˆ°ç»“æœ"}

    block = _format_search_sources(query, results)
    session.append_external_info(block)
    workflow.save_session(session)

    return {
        "success": True,
        "session_id": session_id,
        "query": query,
        "results": results,
        "message": f"å·²å†™å…¥å¤–éƒ¨ä¿¡æ¯ï¼ˆ{len(results)} æ¡æ¥æºï¼‰",
    }


def _extract_json_obj(text: str) -> dict:
    raw = (text or "").strip()
    if not raw:
        return {}
    raw = re.sub(r"^```(?:json)?\\s*", "", raw).strip()
    raw = re.sub(r"\\s*```$", "", raw).strip()
    try:
        obj = json.loads(raw)
        return obj if isinstance(obj, dict) else {}
    except Exception:
        pass

    # Best-effort: grab the largest {...} block
    m = re.search(r"\\{[\\s\\S]*\\}", raw)
    if not m:
        return {}
    try:
        obj = json.loads(m.group(0))
        return obj if isinstance(obj, dict) else {}
    except Exception:
        return {}


def _diagram_storage_dir(session_id: str) -> Path:
    from backend.config import DATA_DIR
    p = Path(DATA_DIR) / "diagrams" / session_id
    p.mkdir(parents=True, exist_ok=True)
    return p


def _diagram_paths(session_id: str, diagram_id: str) -> tuple[Path, Path]:
    d = _diagram_storage_dir(session_id)
    return d / f"{diagram_id}.png", d / f"{diagram_id}.svg"


def _safe_filename(title: str, ext: str) -> str:
    name = (title or "diagram").strip() or "diagram"
    safe = re.sub(r"[^\\u4e00-\\u9fff0-9a-zA-Z._-]+", "_", name).strip("_") or "diagram"
    return f"{safe}.{ext}"


def _diagram_asset_flags(session_id: str, diagram: dict) -> tuple[bool, bool]:
    did = diagram.get("id")
    if not did:
        return False, False
    png_path, svg_path = _diagram_paths(session_id, did)
    has_png = png_path.exists()
    has_svg = svg_path.exists() or bool(diagram.get("has_svg")) or bool(diagram.get("svg"))
    return has_png, has_svg


def _normalize_markdown_text(markdown: str) -> str:
    text = (markdown or "").replace("\r\n", "\n").replace("\r", "\n")
    lines = text.split("\n")
    out: List[str] = []
    blank = 0
    for ln in lines:
        if ln.strip():
            blank = 0
            out.append(ln.rstrip())
            continue
        blank += 1
        if blank <= 2:
            out.append("")
    return "\n".join(out).strip() + "\n"


def _build_figure_markdown(title: str, data_uri: str) -> str:
    t = (title or "å›¾ç¤º").strip() or "å›¾ç¤º"
    return f"![{t}]({data_uri})\n\n*å›¾ï¼š{t}*"


def _split_infographic_points(lines_text: str) -> List[str]:
    source = (lines_text or "").strip()
    if not source:
        return []
    pieces = []
    for line in source.split("\n"):
        s = str(line or "").strip()
        if not s:
            continue
        s = re.sub(r"^\d+[.)ã€]\s*", "", s)
        s = re.sub(r"^[â€¢\-]\s*", "", s)
        s = s.strip("ï¼›; ")
        if s:
            pieces.append(s)
    return pieces[:12]


def _build_local_infographic_spec(
    *,
    diagram_type: str,
    raw_spec: dict,
    lines_text: str,
    title: str,
) -> tuple[str, dict]:
    pieces = _split_infographic_points(lines_text)
    if not pieces:
        pieces = ["ç ”ç©¶ç›®æ ‡ä¸é—®é¢˜å®šä¹‰", "æ–¹æ³•ä¸æŠ€æœ¯è·¯å¾„", "å®éªŒéªŒè¯ä¸æˆæœè¾“å‡º"]

    if diagram_type == "research_framework":
        spec = raw_spec if isinstance(raw_spec, dict) else {}
        if not spec.get("goal"):
            spec["goal"] = {"title": "ç ”ç©¶ç›®æ ‡", "bullets": pieces[:3]}
        if not spec.get("hypotheses"):
            spec["hypotheses"] = {"title": "ç§‘å­¦é—®é¢˜/å‡è®¾", "bullets": pieces[1:4] or pieces[:3]}
        if not spec.get("support"):
            spec["support"] = {"title": "æ”¯æ’‘æ¡ä»¶", "bullets": pieces[2:5] or pieces[:3]}
        if not spec.get("work_packages"):
            spec["work_packages"] = [
                {"title": "WP1 ç ”ç©¶å†…å®¹", "bullets": pieces[:3]},
                {"title": "WP2 ç ”ç©¶å†…å®¹", "bullets": pieces[1:4] or pieces[:3]},
                {"title": "WP3 ç ”ç©¶å†…å®¹", "bullets": pieces[2:5] or pieces[:3]},
            ]
        if not spec.get("outcomes"):
            spec["outcomes"] = {"title": "é¢„æœŸæˆæœ", "bullets": pieces[-3:] or pieces[:3]}
        return "research_framework", spec

    # technical_route / infographic / freestyle -> technical route renderer
    spec = raw_spec if isinstance(raw_spec, dict) else {}
    stages = spec.get("stages")
    if not isinstance(stages, list) or not stages:
        chunk_size = max(2, min(3, len(pieces)))
        split_stages = []
        cursor = 0
        while cursor < len(pieces) and len(split_stages) < 5:
            split_stages.append({
                "title": f"é˜¶æ®µ{len(split_stages) + 1}",
                "bullets": pieces[cursor:cursor + chunk_size],
            })
            cursor += chunk_size
        if len(split_stages) < 3:
            split_stages = [
                {"title": "é˜¶æ®µ1", "bullets": pieces[:3]},
                {"title": "é˜¶æ®µ2", "bullets": pieces[1:4] or pieces[:3]},
                {"title": "é˜¶æ®µ3", "bullets": pieces[-3:] or pieces[:3]},
            ]
        spec["stages"] = split_stages
    spec["title"] = title
    return "technical_route", spec


def _render_local_infographic_assets(
    *,
    diagram_type: str,
    raw_spec: dict,
    lines_text: str,
    title: str,
) -> tuple[bytes, str, dict, str]:
    from backend.core.diagrams.infographic import render_infographic_png_svg

    render_type, render_spec = _build_local_infographic_spec(
        diagram_type=diagram_type,
        raw_spec=raw_spec,
        lines_text=lines_text,
        title=title,
    )
    png_bytes, svg_text, normalized_spec = render_infographic_png_svg(
        render_type,
        render_spec,
        title=title,
    )
    return png_bytes, svg_text, normalized_spec, render_type


def _build_codegen_schema_hint(diagram_type: str) -> str:
    dt = (diagram_type or "").strip().lower()
    if dt == "research_framework":
        return """{
  "goal": {"title": "ç ”ç©¶ç›®æ ‡", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]},
  "hypotheses": {"title": "ç§‘å­¦é—®é¢˜/å‡è®¾", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]},
  "support": {"title": "æ”¯æ’‘æ¡ä»¶", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]},
  "work_packages": [
    {"title": "WP1 ç ”ç©¶å†…å®¹", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]},
    {"title": "WP2 ç ”ç©¶å†…å®¹", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]},
    {"title": "WP3 ç ”ç©¶å†…å®¹", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]}
  ],
  "outcomes": {"title": "é¢„æœŸæˆæœ", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]}
}"""
    return """{
  "stages": [
    {"title": "é˜¶æ®µæ ‡é¢˜", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]},
    {"title": "é˜¶æ®µæ ‡é¢˜", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]},
    {"title": "é˜¶æ®µæ ‡é¢˜", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]},
    {"title": "é˜¶æ®µæ ‡é¢˜", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]}
  ]
}"""


def _dedupe_text_list(items: Any, *, max_items: int = 4, max_len: int = 24) -> List[str]:
    if not isinstance(items, list):
        return []
    out: List[str] = []
    seen: set[str] = set()
    for raw in items:
        text = str(raw or "").strip()
        if not text:
            continue
        text = re.sub(r"\s+", " ", text).strip()
        if len(text) > max_len:
            text = text[:max_len].rstrip()
        key = _normalized_heading_text(text)
        if not key or key in seen:
            continue
        seen.add(key)
        out.append(text)
        if len(out) >= max_items:
            break
    return out


def _extract_codegen_summary_points(payload: dict) -> List[str]:
    if not isinstance(payload, dict):
        return []
    candidates = [
        payload.get("concise_summary"),
        payload.get("summary_points"),
        payload.get("summary"),
        payload.get("outline"),
    ]
    for value in candidates:
        if isinstance(value, list):
            return _dedupe_text_list(value, max_items=8, max_len=22)
        if isinstance(value, str) and value.strip():
            parts = re.split(r"[ï¼›;ã€‚\n]+", value.strip())
            return _dedupe_text_list(parts, max_items=8, max_len=22)
    return []


def _extract_codegen_spec_block(payload: dict) -> dict:
    if not isinstance(payload, dict):
        return {}
    for key in ("diagram_spec", "spec", "structure", "output", "diagram"):
        value = payload.get(key)
        if isinstance(value, dict):
            return value
    return payload


def _sanitize_codegen_spec(spec: dict, diagram_type: str) -> dict:
    dt = (diagram_type or "").strip().lower()
    if not isinstance(spec, dict):
        return {}

    payload = _extract_codegen_spec_block(spec)
    summary_points = _extract_codegen_summary_points(spec)
    global_seen: set[str] = set()
    summary_cursor = 0

    def _borrow_summary(max_items: int = 2) -> List[str]:
        nonlocal summary_cursor
        out: List[str] = []
        if not summary_points:
            return out
        attempts = 0
        while len(out) < max_items and attempts < len(summary_points) * 2:
            candidate = summary_points[summary_cursor % len(summary_points)]
            summary_cursor += 1
            attempts += 1
            key = _normalized_heading_text(candidate)
            if not key or key in global_seen:
                continue
            global_seen.add(key)
            out.append(candidate)
        return out

    def _dedupe_with_global(items: Any, *, max_items: int = 4, max_len: int = 20) -> List[str]:
        local = _dedupe_text_list(items, max_items=max_items * 2, max_len=max_len)
        out: List[str] = []
        for item in local:
            key = _normalized_heading_text(item)
            if not key or key in global_seen:
                continue
            global_seen.add(key)
            out.append(item)
            if len(out) >= max_items:
                break
        return out

    if dt == "research_framework":
        out: Dict[str, Any] = {}
        for key in ("goal", "hypotheses", "support", "outcomes"):
            box = payload.get(key)
            if not isinstance(box, dict):
                continue
            title = str(box.get("title") or "").strip()[:20]
            bullets = _dedupe_with_global(box.get("bullets"), max_items=4, max_len=20)
            if not bullets:
                bullets = _borrow_summary(max_items=2)
            if title or bullets:
                out[key] = {"title": title or key, "bullets": bullets}

        wps = payload.get("work_packages")
        if isinstance(wps, list):
            wp_out = []
            seen_titles: set[str] = set()
            for wp in wps:
                if not isinstance(wp, dict):
                    continue
                title = str(wp.get("title") or "").strip()[:24]
                bullets = _dedupe_with_global(wp.get("bullets"), max_items=4, max_len=20)
                if not bullets:
                    bullets = _borrow_summary(max_items=2)
                key = _normalized_heading_text(title) or _normalized_heading_text(" ".join(bullets))
                if not key or key in seen_titles:
                    continue
                seen_titles.add(key)
                wp_out.append({"title": title or f"WP{len(wp_out) + 1}", "bullets": bullets})
                if len(wp_out) >= 4:
                    break
            if wp_out:
                out["work_packages"] = wp_out

        if not out and summary_points:
            out = {
                "goal": {"title": "ç ”ç©¶ç›®æ ‡", "bullets": summary_points[:3]},
                "hypotheses": {"title": "ç§‘å­¦é—®é¢˜/å‡è®¾", "bullets": summary_points[1:4] or summary_points[:3]},
                "support": {"title": "æ”¯æ’‘æ¡ä»¶", "bullets": summary_points[2:5] or summary_points[:3]},
                "work_packages": [
                    {"title": "WP1 ç ”ç©¶å†…å®¹", "bullets": summary_points[:3]},
                    {"title": "WP2 ç ”ç©¶å†…å®¹", "bullets": summary_points[1:4] or summary_points[:3]},
                    {"title": "WP3 ç ”ç©¶å†…å®¹", "bullets": summary_points[2:5] or summary_points[:3]},
                ],
                "outcomes": {"title": "é¢„æœŸæˆæœ", "bullets": summary_points[-3:] or summary_points[:3]},
            }
        return out

    stages = payload.get("stages")
    if not isinstance(stages, list):
        if summary_points:
            return {
                "stages": [
                    {"title": "é—®é¢˜å®šä¹‰", "bullets": summary_points[:3]},
                    {"title": "æ–¹æ³•è®¾è®¡", "bullets": summary_points[1:4] or summary_points[:3]},
                    {"title": "éªŒè¯ä¸äº§å‡º", "bullets": summary_points[-3:] or summary_points[:3]},
                ]
            }
        return {}
    stage_out = []
    seen_titles: set[str] = set()
    for stage in stages:
        if not isinstance(stage, dict):
            continue
        title = str(stage.get("title") or "").strip()[:20]
        bullets = _dedupe_with_global(stage.get("bullets"), max_items=4, max_len=20)
        if not bullets:
            bullets = _borrow_summary(max_items=2)
        key = _normalized_heading_text(title) or _normalized_heading_text(" ".join(bullets))
        if not key or key in seen_titles:
            continue
        seen_titles.add(key)
        stage_out.append({"title": title or f"é˜¶æ®µ{len(stage_out) + 1}", "bullets": bullets})
        if len(stage_out) >= 6:
            break

    if not stage_out and summary_points:
        stage_out = [
            {"title": "é—®é¢˜å®šä¹‰", "bullets": summary_points[:3]},
            {"title": "æ–¹æ³•è®¾è®¡", "bullets": summary_points[1:4] or summary_points[:3]},
            {"title": "éªŒè¯ä¸äº§å‡º", "bullets": summary_points[-3:] or summary_points[:3]},
        ]
    return {"stages": stage_out} if stage_out else {}


async def _generate_local_infographic_spec_via_skill(
    *,
    workflow,
    diagram_type: str,
    title: str,
    full_context: str,
    focus_context: str,
) -> dict:
    registry = get_registry()
    codegen_skill = registry.get("scientific-infographic-codegen")

    full_text = (full_context or "").strip()
    focus_text = (focus_context or "").strip()
    if len(full_text) > 12000:
        full_text = full_text[:12000]
    if len(focus_text) > 3000:
        focus_text = focus_text[:3000]

    schema_hint = _build_codegen_schema_hint(diagram_type)
    fallback_user_prompt = f"""è¯·ä¸ºç§‘ç ”æ–‡ç¨¿ç”Ÿæˆâ€œ{title}â€çš„ç»“æ„åŒ–å›¾ç¤ºè§„æ ¼ï¼ˆç”¨äºæœ¬åœ°ä»£ç æ¸²æŸ“ï¼Œä¸æ˜¯æˆå›¾æ¨¡å‹ï¼‰ã€‚

åªè¾“å‡º JSONï¼Œä¸è¦è§£é‡Šã€ä¸è¦ä»£ç å—ã€‚
è¯·æŒ‰â€œä¸¤æ­¥æ³•â€ï¼š
æ­¥éª¤1ï¼‰å…ˆåšä¿¡æ¯å‹ç¼©ï¼šä»è¾“å…¥ä¸­æç‚¼ 3-6 æ¡ `concise_summary`ï¼›
æ­¥éª¤2ï¼‰åŸºäºè¯¥æ‘˜è¦è¾“å‡º `diagram_spec`ï¼ˆå¿…é¡»ç¬¦åˆ schemaï¼‰ã€‚

å¼ºçº¦æŸï¼š
1) é‡ç‚¹ç‰‡æ®µä¼˜å…ˆï¼ŒåŒæ—¶ä¸å…¨æ–‡ä¸Šä¸‹æ–‡ä¿æŒä¸€è‡´ï¼›
2) ä¸¥ç¦å¤åˆ¶é•¿æ®µè½ï¼Œå¿…é¡»æ”¹å†™ä¸ºçŸ­è¯­è¦ç‚¹ï¼›
3) æ¯ä¸ª bullets æ¡ç›®å»ºè®® 8-20 å­—ï¼Œé¿å…åŒä¹‰é‡å¤ï¼›
4) æŠ€æœ¯è·¯çº¿å»ºè®®ä½“ç°ï¼šé—®é¢˜å®šä¹‰ â†’ æ–¹æ³•å®ç° â†’ éªŒè¯è¯„ä¼° â†’ äº§å‡ºï¼›
5) ä¸å¾—ç¼–é€ æ•°æ®ã€å®éªŒç»“æœæˆ–ç»“è®ºã€‚

å›¾ç±»å‹ï¼š{diagram_type}

ç›®æ ‡ schemaï¼ˆç”¨äº diagram_specï¼‰ï¼š
{schema_hint}

è¾“å‡º JSON ç›®æ ‡æ ¼å¼ï¼š
{{
  "concise_summary": ["æ‘˜è¦è¦ç‚¹1", "æ‘˜è¦è¦ç‚¹2", "æ‘˜è¦è¦ç‚¹3"],
  "diagram_spec": "æŒ‰ä¸Šæ–¹ schema è¾“å‡ºå¯¹è±¡"
}}

è¾“å…¥Aï¼ˆå…¨æ–‡ä¸Šä¸‹æ–‡ï¼ŒèƒŒæ™¯å‚è€ƒï¼‰ï¼š
{full_text or "ï¼ˆæ— ï¼‰"}

è¾“å…¥Bï¼ˆé‡ç‚¹ç‰‡æ®µï¼Œæ ¸å¿ƒä¼˜å…ˆï¼‰ï¼š
{focus_text or "ï¼ˆæ— ï¼‰"}
"""

    system_prompt = "ä½ æ˜¯ç§‘ç ”å›¾ç¤ºç»“æ„è§„åˆ’å™¨ã€‚åªè¾“å‡º JSONã€‚"
    user_prompt = fallback_user_prompt

    if codegen_skill:
        try:
            system_prompt = _resolve_skill_system_prompt(codegen_skill) or system_prompt
            sections = codegen_skill.get_flat_sections()
            section = sections[0] if sections else None
            if section:
                context = {
                    "requirements": {
                        "diagram_type": diagram_type,
                        "diagram_title": title,
                        "schema_hint": schema_hint,
                        "full_context": full_text,
                        "focus_context": focus_text,
                    },
                    "external_information": full_text,
                }
                rendered = codegen_skill.get_section_prompt(section, context)
                if isinstance(rendered, str) and rendered.strip():
                    user_prompt = rendered.strip()
            if user_prompt == fallback_user_prompt:
                guidelines = (getattr(codegen_skill, "writing_guidelines", "") or "").strip()
                if guidelines:
                    user_prompt = f"{guidelines}\n\n{fallback_user_prompt}"
        except Exception as e:
            logger.warning("codegen skill prompt render failed, fallback prompt used: %s", _redact_secrets(str(e))[:240])

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

    try:
        raw = await workflow.writer_agent._chat(messages, temperature=0.2, max_tokens=900)
        parsed = _extract_json_obj(raw)
        return _sanitize_codegen_spec(parsed, diagram_type)
    except Exception as e:
        logger.warning("local infographic spec generation failed: %s", _redact_secrets(str(e))[:280])
        return {}


def _heading_keywords_for_type(diagram_type: str) -> List[str]:
    dt = (diagram_type or "").strip().lower()
    if dt == "technical_route":
        return ["æŠ€æœ¯è·¯çº¿", "ç ”ç©¶å†…å®¹", "ç ”ç©¶æ–¹æ¡ˆ", "ç ”ç©¶æ–¹æ³•", "æ–¹æ³•", "è·¯çº¿", "å®æ–½æ–¹æ¡ˆ"]
    if dt == "research_framework":
        return ["ç ”ç©¶æ¡†æ¶", "æ€»ä½“æ¡†æ¶", "æ€»ä½“æ€è·¯", "ç ”ç©¶ç›®æ ‡", "ç«‹é¡¹ä¾æ®", "æ¡†æ¶"]
    if dt == "infographic":
        return ["æ‘˜è¦", "æ¦‚è¿°", "å¼•è¨€", "é¡¹ç›®ç®€ä»‹", "ç ”ç©¶æ€è·¯"]
    return ["ç ”ç©¶å†…å®¹", "æ–¹æ³•", "æ¡†æ¶", "ç»“è®º"]


def _normalized_heading_text(text: str) -> str:
    s = re.sub(r"^[#\s]+", "", (text or "").strip())
    s = re.sub(r"[`*_>\-]", "", s)
    s = re.sub(r"\s+", "", s)
    return s.lower()


def _find_heading_index(lines: List[str], heading: str) -> int:
    target = _normalized_heading_text(heading)
    if not target:
        return -1
    for idx, line in enumerate(lines):
        s = line.strip()
        if not s.startswith("#"):
            continue
        h = _normalized_heading_text(s)
        if target in h or h in target:
            return idx
    return -1


def _find_line_index_by_anchor(lines: List[str], anchor_text: str) -> int:
    anchor = (anchor_text or "").strip()
    if not anchor:
        return -1
    for idx, line in enumerate(lines):
        if anchor in line:
            return idx
    return -1


def _fallback_insert_index(lines: List[str], diagram_type: str) -> int:
    keys = _heading_keywords_for_type(diagram_type)
    for idx, line in enumerate(lines):
        s = line.strip()
        if not s.startswith("#"):
            continue
        normalized = _normalized_heading_text(s)
        for key in keys:
            if _normalized_heading_text(key) in normalized:
                return idx

    for idx, line in enumerate(lines):
        if line.strip().startswith("#"):
            return idx
    return 0


async def _plan_illustration_items(
    *,
    workflow,
    document_content: str,
    max_images: int,
    agent_system_prompt: str = "",
) -> List[Dict[str, str]]:
    max_images = max(1, min(int(max_images or 2), 4))
    excerpt = (document_content or "").strip()
    if len(excerpt) > 12000:
        excerpt = excerpt[:12000]

    prompt = f"""ä½ æ˜¯ç§‘ç ”æ–‡æ¡£é…å›¾è§„åˆ’åŠ©æ‰‹ã€‚è¯·æ ¹æ®å…¨æ–‡å†…å®¹ï¼Œè§„åˆ’æœ€å¤š {max_images} å¼ å›¾ã€‚

åªè¾“å‡º JSONï¼š
{{
  "illustrations": [
    {{
      "title": "å›¾æ ‡é¢˜",
      "diagram_type": "technical_route|research_framework|infographic",
      "focus_text": "ä»å…¨æ–‡ä¸­æŒ‘å‡ºçš„å…³é”®å†…å®¹æ‘˜è¦ï¼ˆ1-3å¥ï¼‰"
    }}
  ]
}}

è¦æ±‚ï¼š
1) æ ‡é¢˜ç®€æ´ä¸“ä¸šï¼›
2) ä¼˜å…ˆè¦†ç›–â€œæŠ€æœ¯è·¯çº¿å›¾ã€ç ”ç©¶æ¡†æ¶å›¾â€ï¼›
3) focus_text å¿…é¡»æ¥è‡ªå…¨æ–‡è¯­ä¹‰ï¼Œä¸ç¼–é€ äº‹å®ï¼›
4) ä»…è¾“å‡º JSONã€‚

å…¨æ–‡ï¼š
{excerpt}
"""
    system_prompt = "ä½ åªè¾“å‡º JSONã€‚"
    if agent_system_prompt:
        system_prompt = f"{agent_system_prompt}\n\n{system_prompt}"
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt},
    ]
    try:
        raw = await workflow.writer_agent._chat(messages, temperature=0.2, max_tokens=700)
        obj = _extract_json_obj(raw)
    except Exception:
        obj = {}

    items = obj.get("illustrations") if isinstance(obj, dict) else None
    normalized: List[Dict[str, str]] = []
    if isinstance(items, list):
        for it in items:
            if not isinstance(it, dict):
                continue
            title = str(it.get("title") or "").strip()
            dtype = str(it.get("diagram_type") or "").strip().lower()
            focus_text = str(it.get("focus_text") or "").strip()
            if dtype not in {"technical_route", "research_framework", "infographic"}:
                continue
            if not title:
                title = "æŠ€æœ¯è·¯çº¿å›¾" if dtype == "technical_route" else ("ç ”ç©¶æ¡†æ¶å›¾" if dtype == "research_framework" else "ç§‘ç ”ä¿¡æ¯å›¾")
            if not focus_text:
                focus_text = excerpt[:1200]
            normalized.append({
                "title": title[:40],
                "diagram_type": dtype,
                "focus_text": focus_text[:3000],
            })
            if len(normalized) >= max_images:
                break

    if normalized:
        return normalized

    fallback: List[Dict[str, str]] = [
        {"title": "æŠ€æœ¯è·¯çº¿å›¾", "diagram_type": "technical_route", "focus_text": excerpt[:1800]},
        {"title": "ç ”ç©¶æ¡†æ¶å›¾", "diagram_type": "research_framework", "focus_text": excerpt[:1800]},
    ]
    return fallback[:max_images]


async def _plan_insertions_with_llm(
    *,
    workflow,
    document_content: str,
    diagrams: List[Dict[str, Any]],
    agent_system_prompt: str = "",
) -> List[Dict[str, str]]:
    excerpt = (document_content or "").strip()
    if len(excerpt) > 14000:
        excerpt = excerpt[:14000]

    diag_lines = []
    for d in diagrams:
        diag_lines.append(
            f"- id: {d.get('diagram_id')} | title: {d.get('title')} | type: {d.get('diagram_type')} | focus: {str(d.get('focus_text') or '')[:120]}"
        )
    diag_text = "\n".join(diag_lines) if diag_lines else "ï¼ˆæ— ï¼‰"

    prompt = f"""ä½ æ˜¯ç§‘ç ”æ–‡æ¡£æ’ç‰ˆåŠ©æ‰‹ã€‚è¯·ä¸ºæ¯å¼ å›¾ç¡®å®šåœ¨å…¨æ–‡ä¸­çš„æ’å…¥ä½ç½®ã€‚

åªè¾“å‡º JSONï¼š
{{
  "placements": [
    {{
      "diagram_id": "å›¾ID",
      "anchor_heading": "ä¼˜å…ˆæ’å…¥åˆ°æ­¤æ ‡é¢˜ä¹‹åï¼ˆå¯ä¸ºç©ºï¼‰",
      "anchor_text": "è‹¥æ— æ ‡é¢˜åˆ™åŒ¹é…è¯¥å¥å­ç‰‡æ®µï¼ˆå¯ä¸ºç©ºï¼‰"
    }}
  ]
}}

è§„åˆ™ï¼š
1) è®©å›¾ç‰‡è´´è¿‘å¯¹åº”ç« èŠ‚ï¼Œé¿å…å…¨éƒ¨å †åœ¨æ–‡æ¡£å¼€å¤´ï¼›
2) ä¼˜å…ˆä½¿ç”¨æ ‡é¢˜é”šç‚¹ï¼›
3) ä¸æ”¹å˜æ­£æ–‡åŸæ„ï¼›
4) ä»…è¾“å‡º JSONã€‚

å¾…æ’å…¥å›¾ç‰‡ï¼š
{diag_text}

å…¨æ–‡ Markdownï¼š
{excerpt}
"""
    system_prompt = "ä½ åªè¾“å‡º JSONã€‚"
    if agent_system_prompt:
        system_prompt = f"{agent_system_prompt}\n\n{system_prompt}"
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt},
    ]
    try:
        raw = await workflow.writer_agent._chat(messages, temperature=0.1, max_tokens=700)
        obj = _extract_json_obj(raw)
    except Exception:
        obj = {}

    placements = obj.get("placements") if isinstance(obj, dict) else None
    out: List[Dict[str, str]] = []
    if isinstance(placements, list):
        for p in placements:
            if not isinstance(p, dict):
                continue
            did = str(p.get("diagram_id") or "").strip()
            if not did:
                continue
            out.append({
                "diagram_id": did,
                "anchor_heading": str(p.get("anchor_heading") or "").strip(),
                "anchor_text": str(p.get("anchor_text") or "").strip(),
            })
    return out


def _apply_diagram_insertions(
    *,
    document_content: str,
    diagrams: List[Dict[str, Any]],
    placements: List[Dict[str, str]],
) -> str:
    text = _normalize_markdown_text(document_content or "")
    lines = text.split("\n")

    placement_map = {str(p.get("diagram_id") or "").strip(): p for p in placements if isinstance(p, dict)}

    indexed_blocks: List[tuple[int, int, str]] = []
    for order, d in enumerate(diagrams):
        did = str(d.get("diagram_id") or "").strip()
        if not did:
            continue
        placement = placement_map.get(did, {})
        idx = -1
        anchor_heading = str(placement.get("anchor_heading") or "").strip()
        if anchor_heading:
            idx = _find_heading_index(lines, anchor_heading)
        if idx < 0:
            anchor_text = str(placement.get("anchor_text") or "").strip()
            if anchor_text:
                idx = _find_line_index_by_anchor(lines, anchor_text)
        if idx < 0:
            idx = _fallback_insert_index(lines, str(d.get("diagram_type") or ""))

        snippet = str(d.get("markdown_snippet") or "").strip()
        if not snippet:
            continue
        block_lines = ["", snippet, ""]
        block = "\n".join(block_lines)
        indexed_blocks.append((idx, order, block))

    # Desc insert to keep indices stable
    indexed_blocks.sort(key=lambda x: (x[0], x[1]), reverse=True)
    for idx, _order, block in indexed_blocks:
        insert_at = max(0, min(idx + 1, len(lines)))
        b_lines = block.split("\n")
        lines[insert_at:insert_at] = b_lines

    return _normalize_markdown_text("\n".join(lines))


@router.post("/session/{session_id}/generate-diagram")
async def generate_diagram(session_id: str, request: DiagramRequest):
    """ç”Ÿæˆå›¾ç¤ºï¼š`image_model` èµ°æˆå›¾æ¨¡å‹ï¼›`infographic` èµ°æœ¬åœ°ä¿¡æ¯å›¾æ¸²æŸ“ï¼ˆæ— å¤–éƒ¨æˆå›¾ä¾èµ–ï¼‰ã€‚"""
    workflow = get_workflow()
    session = workflow.get_session(session_id)
    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    _ensure_llm_configured()

    registry = get_registry()
    skill = registry.get(session.skill_id)
    if not skill:
        raise HTTPException(status_code=404, detail=f"Skill not found: {session.skill_id}")

    diagram_type = (request.diagram_type or "technical_route").strip().lower() or "technical_route"
    if diagram_type not in {"technical_route", "research_framework", "freestyle", "infographic"}:
        raise HTTPException(status_code=400, detail=f"Unsupported diagram_type: {diagram_type}")

    mode = (request.mode or "infographic").strip().lower() or "infographic"
    if mode not in {"infographic", "image_model", "auto"}:
        raise HTTPException(status_code=400, detail=f"Unsupported mode: {mode}")
    if mode == "auto":
        mode = "infographic"

    cfg = get_llm_config()
    image_model = (getattr(cfg, "image_model", None) or "").strip()
    if mode == "image_model":
        if not image_model:
            raise HTTPException(status_code=400, detail="æœªé…ç½®æˆå›¾æ¨¡å‹ï¼šè¯·å…ˆåœ¨è®¾ç½®ä¸­å¡«å†™â€œæˆå›¾æ¨¡å‹ï¼ˆImage Modelï¼‰â€ã€‚")
        if not (cfg.base_url or "").strip():
            raise HTTPException(status_code=400, detail="æœªé…ç½® Base URLï¼Œæ— æ³•è°ƒç”¨æˆå›¾æ¨¡å‹ã€‚")

    default_title = {
        "technical_route": "æŠ€æœ¯è·¯çº¿å›¾",
        "research_framework": "ç ”ç©¶æ¡†æ¶å›¾",
        "freestyle": "ç§‘ç ”ç¤ºæ„å›¾",
        "infographic": "ç§‘ç ”ä¿¡æ¯å›¾",
    }
    title = (request.title or default_title.get(diagram_type, "å›¾ç¤º")).strip()

    if request.selected_text:
        selected_text = (request.selected_text or "").strip()
        requirements_text = selected_text or "ï¼ˆæš‚æ— ï¼‰"
        external_excerpt = (request.context_text or "").strip()
        if len(external_excerpt) > 12000:
            external_excerpt = external_excerpt[:12000]
    else:
        requirements = session.requirements or {}
        req_lines = []
        for f in skill.requirement_fields:
            v = requirements.get(f.id)
            if v is None:
                continue
            if isinstance(v, str) and not v.strip():
                continue
            req_lines.append(f"- {f.name}({f.id}): {v}")
        requirements_text = "\n".join(req_lines) if req_lines else "ï¼ˆæš‚æ— ï¼‰"
        external_excerpt = (session.external_information or "").strip()
        if len(external_excerpt) > 2500:
            external_excerpt = external_excerpt[:2500]

    raw_spec = {}
    full_context_for_codegen = (request.context_text or external_excerpt or "").strip()
    focus_context_for_codegen = (request.selected_text or requirements_text or "").strip()

    if mode == "infographic":
        raw_spec = await _generate_local_infographic_spec_via_skill(
            workflow=workflow,
            diagram_type=diagram_type,
            title=title,
            full_context=full_context_for_codegen,
            focus_context=focus_context_for_codegen,
        )
    elif diagram_type in {"technical_route", "research_framework"}:
        if diagram_type == "technical_route":
            json_schema_hint = """{
  "stages": [
    {"title": "é˜¶æ®µæ ‡é¢˜", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]},
    {"title": "é˜¶æ®µæ ‡é¢˜", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]},
    {"title": "é˜¶æ®µæ ‡é¢˜", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]},
    {"title": "é˜¶æ®µæ ‡é¢˜", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]}
  ]
}"""
            content_rule = "æŠ€æœ¯è·¯çº¿å›¾ï¼šå»ºè®® 4 ä¸ªé˜¶æ®µï¼Œä½“ç°â€œç›®æ ‡/é—®é¢˜ â†’ æ–¹æ³• â†’ å®éªŒéªŒè¯ â†’ äº§å‡ºè¯„ä¼°â€çš„é—­ç¯ã€‚"
        else:
            json_schema_hint = """{
  "goal": {"title": "ç ”ç©¶ç›®æ ‡", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]},
  "hypotheses": {"title": "ç§‘å­¦é—®é¢˜/å‡è®¾", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]},
  "support": {"title": "æ”¯æ’‘æ¡ä»¶", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]},
  "work_packages": [
    {"title": "WP1 ç ”ç©¶å†…å®¹", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]},
    {"title": "WP2 ç ”ç©¶å†…å®¹", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]},
    {"title": "WP3 ç ”ç©¶å†…å®¹", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]}
  ],
  "outcomes": {"title": "é¢„æœŸæˆæœ", "bullets": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]}
}"""
            content_rule = "ç ”ç©¶æ¡†æ¶å›¾ï¼šå¼ºè°ƒâ€œç›®æ ‡/å‡è®¾ â†’ ä»»åŠ¡åŒ…(WP) â†’ é¢„æœŸæˆæœâ€ï¼Œå¹¶æ ‡å‡ºæ”¯æ’‘æ¡ä»¶ã€‚"

        prompt = f"""ä½ æ˜¯ç§‘ç ”ç”³æŠ¥ä¹¦å›¾ç¤ºè®¾è®¡åŠ©æ‰‹ã€‚è¯·æ ¹æ®â€œå·²çŸ¥éœ€æ±‚â€å’Œâ€œææ–™æ‘˜è¦â€ï¼Œç”Ÿæˆå¯ç”¨äºç»˜åˆ¶â€œ{title}â€çš„ç»“æ„åŒ– JSONã€‚

åªè¾“å‡º JSONï¼ˆä¸è¦è§£é‡Šã€ä¸è¦ Markdown ä»£ç å—ï¼‰ã€‚

ç¡¬æ€§çº¦æŸï¼š
1) ç»“æ„å¿…é¡»ä¸¥æ ¼åŒ¹é…ä¸‹é¢çš„ JSON ç»“æ„ç¤ºä¾‹ï¼ˆå­—æ®µåä¿æŒä¸€è‡´ï¼‰ï¼›
2) ä¸å¾—è™šæ„æ•°æ®ã€ç»“æœã€å®éªŒç»“è®ºï¼›
3) æ¯ä¸ª title <= 14 å­—ï¼›æ¯æ¡ bullets <= 22 å­—ï¼›
4) ä¿¡æ¯ä¸è¶³æ—¶ï¼Œä½¿ç”¨è§„èŒƒä½†ä¿å®ˆçš„æè¿°ã€‚

å†…å®¹è§„åˆ™ï¼š{content_rule}
æ–‡ä¹¦ç±»å‹ï¼š{skill.metadata.name}

å·²çŸ¥éœ€æ±‚ï¼š
{requirements_text}

ææ–™æ‘˜è¦ï¼ˆå¯ä¸ºç©ºï¼‰ï¼š
{external_excerpt if external_excerpt else "ï¼ˆæ— ï¼‰"}

JSON ç»“æ„ç¤ºä¾‹ï¼š
{json_schema_hint}
"""
        messages = [
            {"role": "system", "content": "ä½ åªè¾“å‡º JSONã€‚"},
            {"role": "user", "content": prompt},
        ]
        try:
            spec_text = await workflow.writer_agent._chat(messages, temperature=0.2, max_tokens=900)
            raw_spec = _extract_json_obj(spec_text)
        except Exception as e:
            logger.warning("diagram spec generation failed: %s", _redact_secrets(str(e))[:240])
            raw_spec = {}

    def _lines_from_spec() -> str:
        if diagram_type == "technical_route":
            stages = (raw_spec.get("stages") if isinstance(raw_spec, dict) else None) or []
            out = []
            for i, st in enumerate(stages[:6] if isinstance(stages, list) else []):
                if not isinstance(st, dict):
                    continue
                t = str(st.get("title") or f"é˜¶æ®µ{i + 1}").strip()
                bs = st.get("bullets") or []
                btxt = "ï¼›".join([str(x).strip() for x in (bs[:4] if isinstance(bs, list) else []) if str(x).strip()])
                if t or btxt:
                    out.append(f"{i + 1}. {t}ï¼š{btxt}".strip("ï¼š"))
            return "\n".join(out) if out else requirements_text

        if diagram_type == "research_framework":
            parts = []
            for key in ("goal", "hypotheses", "support", "outcomes"):
                box = raw_spec.get(key) if isinstance(raw_spec, dict) else None
                if not isinstance(box, dict):
                    continue
                t = str(box.get("title") or "").strip()
                bs = box.get("bullets") or []
                btxt = "ï¼›".join([str(x).strip() for x in (bs[:4] if isinstance(bs, list) else []) if str(x).strip()])
                if t or btxt:
                    parts.append(f"{t}ï¼š{btxt}".strip("ï¼š"))
            wps = raw_spec.get("work_packages") if isinstance(raw_spec, dict) else None
            if isinstance(wps, list):
                for i, wp in enumerate(wps[:3]):
                    if not isinstance(wp, dict):
                        continue
                    t = str(wp.get("title") or f"WP{i + 1}").strip()
                    bs = wp.get("bullets") or []
                    btxt = "ï¼›".join([str(x).strip() for x in (bs[:4] if isinstance(bs, list) else []) if str(x).strip()])
                    if t or btxt:
                        parts.append(f"{t}ï¼š{btxt}".strip("ï¼š"))
            return "\n".join(parts) if parts else requirements_text

        source = ((request.selected_text or "").strip() or requirements_text or "").strip()
        pieces = [x.strip("â€¢- \t") for x in re.split(r"[\nï¼›;]+", source) if x and x.strip("â€¢- \t")]
        if not pieces:
            return "1. ç ”ç©¶ç›®æ ‡ä¸é—®é¢˜å®šä¹‰\n2. æ–¹æ³•ä¸æŠ€æœ¯è·¯å¾„\n3. éªŒè¯ä¸æˆæœè¾“å‡º"
        return "\n".join([f"{i + 1}. {v}" for i, v in enumerate(pieces[:10])])

    if diagram_type == "technical_route":
        diagram_kind_cn = "æŠ€æœ¯è·¯çº¿å›¾"
        layout_hint = "æŒ‰é˜¶æ®µä»å·¦åˆ°å³æˆ–ä»ä¸Šåˆ°ä¸‹ï¼Œ4-6 ä¸ªé˜¶æ®µï¼Œç®­å¤´æ¸…æ™°è¡¨è¾¾å…ˆåä¾èµ–ä¸åé¦ˆé—­ç¯ã€‚"
    elif diagram_type == "research_framework":
        diagram_kind_cn = "ç ”ç©¶æ¡†æ¶å›¾"
        layout_hint = "é‡‡ç”¨â€œç›®æ ‡/é—®é¢˜ â†’ ä»»åŠ¡åŒ…(WP) â†’ éªŒè¯/æˆæœâ€çš„å±‚æ¬¡ç»“æ„ï¼Œå¹¶æ ‡å‡ºæ”¯æ’‘æ¡ä»¶ã€‚"
    elif diagram_type == "freestyle":
        diagram_kind_cn = "ç§‘ç ”ç¤ºæ„å›¾"
        layout_hint = "æ ¹æ®å†…å®¹ç»„ç»‡æˆé€»è¾‘æ¸…æ™°çš„æ¨¡å—å…³ç³»å›¾ï¼Œé¿å…è£…é¥°æ€§å…ƒç´ ã€‚"
    else:
        diagram_kind_cn = "ç§‘ç ”ä¿¡æ¯å›¾"
        layout_hint = "ä»¥ä¿¡æ¯å›¾æ ·å¼è¡¨è¾¾å…³é”®æ¨¡å—å’Œå…³ç³»ï¼Œå¼ºè°ƒç»“æ„åŒ–ã€å¯è¯»æ€§ã€å¯è§£é‡Šæ€§ã€‚"

    if mode == "infographic":
        style_hint = (
            "ä¿¡æ¯å›¾é£æ ¼ï¼ˆinfographicï¼‰ï¼šæ‰å¹³åŒ–ã€å¡ç‰‡åˆ†åŒºã€ç»Ÿä¸€é…è‰²ï¼›èƒŒæ™¯ç™½è‰²æˆ–æµ…è‰²ï¼›"
            "ä¸­æ–‡æ–‡å­—æ¸…æ™°ï¼›ç¦æ­¢æ°´å°ã€Logoã€æ‘„å½±é£ã€‚"
        )
    else:
        style_hint = (
            "ç§‘ç ”å›¾ç¤ºé£æ ¼ï¼šä¸“ä¸šã€å…‹åˆ¶ã€ç»“æ„å¯¼å‘ï¼›ä»¥æ¨¡å—æ¡†å›¾å’Œæµç¨‹ç®­å¤´ä¸ºä¸»ï¼›"
            "ä¸­æ–‡æ ‡ç­¾æ¸…æ™°ã€æœ¯è¯­è§„èŒƒï¼›ç¦æ­¢æ°´å°ã€Logoã€‚"
        )

    lines_text = _lines_from_spec()
    image_prompt = f"""è¯·ç”Ÿæˆä¸€å¼ å¯ç›´æ¥ç”¨äºç§‘ç ”ç”³æŠ¥ä¹¦çš„ä¸­æ–‡{diagram_kind_cn}ã€‚

é£æ ¼è¦æ±‚ï¼š
- {style_hint}
- {layout_hint}
- å›¾ä¸­æ–‡å­—å¿…é¡»æ¸…æ™°å¯è¯»ï¼Œé¿å…è¿‡åº¦æ‹¥æŒ¤ä¸é‡å ã€‚

æ ‡é¢˜ï¼š{title}
å†…å®¹ï¼ˆä¿æŒå«ä¹‰ï¼Œä¸å¾—è™šæ„æ•°æ®/ç»“è®ºï¼‰ï¼š
{lines_text}
"""

    review_prompt = f"""è¯·å®¡æ ¸è¿™å¼ å›¾æ˜¯å¦æ»¡è¶³ç§‘ç ”ç”³æŠ¥ä¹¦å¯ç”¨æ ‡å‡†ï¼Œå¹¶ä¸¥æ ¼è¾“å‡º JSONï¼š
{{
  "passed": true/false,
  "score": 0-100,
  "issues": ["é—®é¢˜1", "é—®é¢˜2"],
  "improvements": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"],
  "summary": "ä¸€å¥è¯ç»“è®º"
}}

å®¡æ ¸ç»´åº¦ï¼š
1) ç»“æ„å®Œæ•´æ€§ï¼šæµç¨‹/å±‚æ¬¡æ˜¯å¦æ¸…æ™°ï¼Œå…³ç³»æ˜¯å¦æ˜ç¡®ï¼›
2) å­¦æœ¯å¯ç”¨æ€§ï¼šæœ¯è¯­æ˜¯å¦è§„èŒƒï¼Œæ˜¯å¦é¿å…ä¸å®ç»“è®ºï¼›
3) å¯è¯»æ€§ï¼šä¸­æ–‡æ˜¯å¦æ¸…æ™°ï¼Œæ˜¯å¦æ‹¥æŒ¤æˆ–é‡å ï¼›
4) å›¾ç¤ºè´¨é‡ï¼šå¸ƒå±€ã€å¯¹é½ã€è§†è§‰ä¸€è‡´æ€§ã€‚
"""

    actual_mode = mode
    review_result = None
    review_error = None
    svg_text = ""
    used_spec = raw_spec or {}

    if mode == "infographic":
        try:
            png_bytes, svg_text, used_spec, render_type = _render_local_infographic_assets(
                diagram_type=diagram_type,
                raw_spec=raw_spec,
                lines_text=lines_text,
                title=title,
            )
            review_result = {
                "passed": True,
                "score": 95,
                "issues": [],
                "improvements": [],
                "summary": f"å·²é€šè¿‡æœ¬åœ°ä¿¡æ¯å›¾å¼•æ“ç”Ÿæˆï¼ˆ{render_type}ï¼‰ã€‚",
            }
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"æœ¬åœ°ä¿¡æ¯å›¾ç”Ÿæˆå¤±è´¥ï¼š{_redact_secrets(str(e))}")
    else:
        from backend.core.diagrams.openai_images import (
            generate_image_png_via_openai_compatible,
            review_image_via_openai_compatible,
        )

        async def _generate_image(prompt_text: str) -> bytes:
            last_err = None
            for size in ("1792x1024", "1024x1024"):
                try:
                    png, _raw = await generate_image_png_via_openai_compatible(
                        base_url=cfg.base_url,
                        api_key=cfg.api_key,
                        model=image_model,
                        prompt=prompt_text,
                        size=size,
                    )
                    return png
                except Exception as e:
                    last_err = e
                    continue
            if last_err is None:
                raise RuntimeError("æˆå›¾æ¨¡å‹æœªè¿”å›å›¾åƒ")
            raise last_err

        async def _review_image(png: bytes) -> dict:
            review, _raw = await review_image_via_openai_compatible(
                base_url=cfg.base_url,
                api_key=cfg.api_key,
                model=image_model,
                review_prompt=review_prompt,
                image_bytes=png,
            )
            return review

        try:
            png_bytes = await _generate_image(image_prompt)
        except Exception as e:
            # image_model ä¸ç¨³å®šæ—¶ï¼Œè‡ªåŠ¨å›é€€åˆ°æœ¬åœ° infographicï¼Œä¿è¯åŠŸèƒ½å¯ç”¨
            gen_error = _redact_secrets(str(e))
            logger.warning("image model generation failed, fallback to local infographic: %s", gen_error[:320])
            try:
                png_bytes, svg_text, used_spec, render_type = _render_local_infographic_assets(
                    diagram_type=diagram_type,
                    raw_spec=raw_spec,
                    lines_text=lines_text,
                    title=title,
                )
                actual_mode = "infographic"
                review_error = gen_error
                review_result = {
                    "passed": True,
                    "score": 88,
                    "issues": ["æˆå›¾æ¨¡å‹æ¥å£ä¸å¯ç”¨ï¼Œå·²è‡ªåŠ¨é™çº§ä¸ºæœ¬åœ°ä¿¡æ¯å›¾æ¸²æŸ“ã€‚"],
                    "improvements": ["å¦‚éœ€ç…§ç‰‡çº§è§†è§‰é£æ ¼ï¼Œè¯·æ¢å¤ image_model æœåŠ¡åé‡è¯•ã€‚"],
                    "summary": f"å·²è‡ªåŠ¨å›é€€åˆ°æœ¬åœ°ä¿¡æ¯å›¾å¼•æ“ï¼ˆ{render_type}ï¼‰ã€‚",
                }
            except Exception as fallback_e:
                raise HTTPException(
                    status_code=502,
                    detail=f"æˆå›¾æ¨¡å‹ç”Ÿæˆå¤±è´¥ï¼š{gen_error}ï¼›æœ¬åœ°å›é€€ä¹Ÿå¤±è´¥ï¼š{_redact_secrets(str(fallback_e))}",
                )

        if review_result is None:
            try:
                review_result = await _review_image(png_bytes)
            except Exception as e:
                review_error = _redact_secrets(str(e))
                logger.warning("diagram review failed: %s", review_error[:280])

        if isinstance(review_result, dict) and not review_result.get("passed", True):
            suggestions = review_result.get("improvements") or review_result.get("issues") or []
            if isinstance(suggestions, list) and suggestions:
                suggestion_text = "\n".join([f"- {str(x).strip()}" for x in suggestions if str(x).strip()][:8])
                retry_prompt = f"""{image_prompt}

è¯·æ ¹æ®ä»¥ä¸‹å®¡æ ¸æ„è§é‡ç»˜å¹¶æå‡å›¾ç¤ºè´¨é‡ï¼ˆä¿æŒåŸæœ‰è¯­ä¹‰ï¼Œä¸å¾—è™šæ„ï¼‰ï¼š
{suggestion_text}
"""
                try:
                    revised = await _generate_image(retry_prompt)
                    png_bytes = revised
                    try:
                        review_result = await _review_image(png_bytes)
                        review_error = None
                    except Exception as e:
                        review_error = _redact_secrets(str(e))
                except Exception as e:
                    logger.warning("diagram retry generation failed: %s", _redact_secrets(str(e))[:260])

    diagram_id = str(uuid.uuid4())
    png_path, svg_path = _diagram_paths(session_id, diagram_id)

    try:
        png_path.write_bytes(png_bytes)
        if svg_text:
            svg_path.write_text(svg_text, encoding="utf-8")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä¿å­˜å›¾ç¤ºå¤±è´¥ï¼š{_redact_secrets(str(e))}")

    b64_png = base64.b64encode(png_bytes).decode("ascii")
    data_uri = f"data:image/png;base64,{b64_png}"
    snippet = _build_figure_markdown(title, data_uri)

    diagram_record = {
        "id": diagram_id,
        "title": title,
        "diagram_type": diagram_type,
        "mode": actual_mode,
        "created_at": datetime.datetime.now().isoformat(timespec="seconds"),
        "has_png": True,
        "has_svg": bool(svg_text),
        "spec": used_spec or None,
        "meta": {
            "provider": getattr(cfg, "provider_name", None),
            "chat_model": getattr(cfg, "model", None),
            "image_model": image_model or None,
            "review_model": image_model or None,
            "render_engine": "local_infographic" if svg_text else "image_model",
            "review": review_result,
            "review_error": review_error,
        },
    }
    session.diagrams = (session.diagrams or []) + [diagram_record]
    workflow.save_session(session)

    return {
        "success": True,
        "session_id": session_id,
        "diagram_id": diagram_id,
        "title": title,
        "diagram_type": diagram_type,
        "mode": actual_mode,
        "image_data_uri": data_uri,
        "markdown_snippet": snippet,
        "has_svg": bool(svg_text),
        "png_url": f"/api/chat/session/{session_id}/diagrams/{diagram_id}.png",
        "svg_url": f"/api/chat/session/{session_id}/diagrams/{diagram_id}.svg" if svg_text else None,
        "review": review_result,
        "review_error": review_error,
    }


@router.post("/session/{session_id}/generate-illustrations")
async def generate_illustrations(session_id: str, request: GenerateIllustrationsRequest):
    """
    æ–‡ç« é…å›¾ Agentï¼š
    1) åŸºäºå…¨æ–‡è§„åˆ’è¦ç”Ÿæˆçš„å›¾ï¼›
    2) ç”Ÿæˆå¹¶å®¡æ ¸å›¾ï¼›
    3) æœ€åå†è°ƒç”¨ä¸€æ¬¡ LLM è§„åˆ’æ’å…¥ä½ç½®å¹¶å®Œæˆæ’å…¥ã€‚
    """
    workflow = get_workflow()
    session = workflow.get_session(session_id)
    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")
    if session.phase != "complete":
        raise HTTPException(status_code=400, detail="å…¨æ–‡å°šæœªç”Ÿæˆå®Œæˆï¼Œè¯·å…ˆå®Œæˆæ–‡æ¡£ç”Ÿæˆåå†è§¦å‘é…å›¾ã€‚")

    _ensure_llm_configured()

    document_content = _normalize_markdown_text(request.document_content or "")
    if len(document_content.strip()) < 30:
        raise HTTPException(status_code=400, detail="æ–‡ç« å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•è‡ªåŠ¨é…å›¾ã€‚")

    mode = (request.mode or "infographic").strip().lower() or "infographic"
    if mode not in {"infographic", "image_model", "auto"}:
        raise HTTPException(status_code=400, detail=f"Unsupported mode: {mode}")
    if mode == "auto":
        mode = "infographic"

    max_images = max(1, min(int(request.max_images or 2), 4))

    registry = get_registry()
    schematics_skill = registry.get("scientific-schematics")
    agent_system_prompt = _resolve_skill_system_prompt(schematics_skill)
    if len(agent_system_prompt) > 4000:
        agent_system_prompt = agent_system_prompt[:4000]

    # Step 1) å…ˆåŸºäºå…¨æ–‡è§„åˆ’é…å›¾ä»»åŠ¡
    items = await _plan_illustration_items(
        workflow=workflow,
        document_content=document_content,
        max_images=max_images,
        agent_system_prompt=agent_system_prompt,
    )

    # Step 2) é€å›¾ç”Ÿæˆï¼ˆä»å¤ç”¨ generate-diagram ä¸»é“¾è·¯ï¼Œselected_text ä¸ºæ ¸å¿ƒï¼Œcontext_text ä¸ºå…¨æ–‡ï¼‰
    created: List[Dict[str, Any]] = []
    for item in items:
        try:
            result = await generate_diagram(
                session_id=session_id,
                request=DiagramRequest(
                    title=item.get("title") or None,
                    diagram_type=item.get("diagram_type") or "infographic",
                    mode=mode,
                    selected_text=(item.get("focus_text") or "").strip() or document_content[:1600],
                    context_text=document_content,
                ),
            )
            created.append({
                "diagram_id": result.get("diagram_id"),
                "title": result.get("title"),
                "diagram_type": result.get("diagram_type"),
                "markdown_snippet": result.get("markdown_snippet"),
                "focus_text": item.get("focus_text") or "",
                "review": result.get("review"),
                "review_error": result.get("review_error"),
            })
        except Exception as e:
            logger.warning("auto illustration generate failed: %s", _redact_secrets(str(e))[:260])
            continue

    if not created:
        raise HTTPException(status_code=502, detail="è‡ªåŠ¨é…å›¾å¤±è´¥ï¼šæœªç”Ÿæˆä»»ä½•å›¾ç‰‡ã€‚")

    # Step 3) æœ€åä¸€è½® LLMï¼šå†³å®šæ’å›¾ä½ç½®
    placements = await _plan_insertions_with_llm(
        workflow=workflow,
        document_content=document_content,
        diagrams=created,
        agent_system_prompt=agent_system_prompt,
    )

    # Step 4) æ’å…¥å¹¶ä¼˜åŒ–æ’ç‰ˆ
    updated_document = _apply_diagram_insertions(
        document_content=document_content,
        diagrams=created,
        placements=placements,
    )

    return {
        "success": True,
        "session_id": session_id,
        "mode": mode,
        "inserted_count": len(created),
        "updated_document": updated_document,
        "diagrams": created,
        "placements": placements,
    }


@router.get("/session/{session_id}/diagrams")
async def list_diagrams(session_id: str):
    """List session diagrams (metadata only)."""
    workflow = get_workflow()
    session = workflow.get_session(session_id)
    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    diagrams = session.diagrams or []
    return {
        "session_id": session_id,
        "diagrams": [
            {
                "id": d.get("id"),
                "title": d.get("title"),
                "diagram_type": d.get("diagram_type"),
                "mode": d.get("mode") or ("infographic" if d.get("svg") else "unknown"),
                "created_at": d.get("created_at"),
                "has_png": (flags := _diagram_asset_flags(session_id, d))[0],
                "has_svg": flags[1],
                "png_url": f"/api/chat/session/{session_id}/diagrams/{d.get('id')}.png" if flags[0] else None,
                "svg_url": f"/api/chat/session/{session_id}/diagrams/{d.get('id')}.svg" if flags[1] else None,
            }
            for d in diagrams
            if isinstance(d, dict)
        ],
    }


@router.get("/session/{session_id}/diagrams/{diagram_id}.svg")
async def get_diagram_svg(session_id: str, diagram_id: str):
    """Download a stored diagram SVG (infographic mode)."""
    workflow = get_workflow()
    session = workflow.get_session(session_id)
    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    diagram = next((d for d in (session.diagrams or []) if isinstance(d, dict) and d.get("id") == diagram_id), None)
    if not diagram:
        raise HTTPException(status_code=404, detail="Diagram not found")

    # New format: file-based SVG
    _png_path, svg_path = _diagram_paths(session_id, diagram_id)
    if svg_path.exists():
        svg = svg_path.read_text(encoding="utf-8", errors="ignore")
        filename = _safe_filename(diagram.get("title") or "diagram", "svg")
        return StreamingResponse(
            iter([svg.encode("utf-8")]),
            media_type="image/svg+xml",
            headers={"Content-Disposition": f'attachment; filename="{filename}"'},
        )

    # Backward compatibility: stored inline SVG string (older mermaid-based diagrams)
    svg_inline = (diagram.get("svg") or "").strip()
    if svg_inline:
        filename = _safe_filename(diagram.get("title") or "diagram", "svg")
        return StreamingResponse(
            iter([svg_inline.encode("utf-8")]),
            media_type="image/svg+xml",
            headers={"Content-Disposition": f'attachment; filename="{filename}"'},
        )

    raise HTTPException(status_code=404, detail="SVG ä¸å­˜åœ¨ï¼ˆè¯¥å›¾ç¤ºå¯èƒ½ç”±æˆå›¾æ¨¡å‹ç”Ÿæˆï¼‰")


@router.get("/session/{session_id}/diagrams/{diagram_id}.png")
async def get_diagram_png(session_id: str, diagram_id: str):
    """Download a stored diagram PNG."""
    workflow = get_workflow()
    session = workflow.get_session(session_id)
    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    diagram = next((d for d in (session.diagrams or []) if isinstance(d, dict) and d.get("id") == diagram_id), None)
    if not diagram:
        raise HTTPException(status_code=404, detail="Diagram not found")

    png_path, _svg_path = _diagram_paths(session_id, diagram_id)
    if not png_path.exists():
        raise HTTPException(status_code=404, detail="PNG ä¸å­˜åœ¨")

    raw = png_path.read_bytes()
    filename = _safe_filename(diagram.get("title") or "diagram", "png")
    return StreamingResponse(
        iter([raw]),
        media_type="image/png",
        headers={"Content-Disposition": f'attachment; filename="{filename}"'},
    )


class UpdateRequirementsRequest(BaseModel):
    """æ›´æ–°éœ€æ±‚è¯·æ±‚"""
    requirements: dict


@router.put("/session/{session_id}/requirements")
async def update_requirements(session_id: str, request: UpdateRequirementsRequest):
    """
    ç›´æ¥æ›´æ–°ä¼šè¯çš„éœ€æ±‚å­—æ®µ

    - ç”¨äºè¡¨å•ç›´æ¥ç¼–è¾‘éœ€æ±‚
    - ä¸éœ€è¦é€šè¿‡å¯¹è¯æ”¶é›†
    """
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    # æ›´æ–°éœ€æ±‚
    if session.requirements is None:
        session.requirements = {}

    # åˆå¹¶æ–°çš„éœ€æ±‚ï¼ˆä¿ç•™å·²æœ‰å€¼ï¼Œé™¤éæ˜ç¡®è¦†ç›–ï¼‰
    for key, value in request.requirements.items():
        if value is None:
            session.requirements.pop(key, None)
            continue
        if isinstance(value, str) and not value.strip():
            session.requirements.pop(key, None)
            continue
        session.requirements[key] = value

    # ä¿å­˜ä¼šè¯
    workflow.save_session(session)

    return {
        "success": True,
        "session_id": session_id,
        "requirements": session.requirements,
    }


@router.get("/session/{session_id}/requirements")
async def get_requirements(session_id: str):
    """è·å–ä¼šè¯çš„éœ€æ±‚å­—æ®µ"""
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    # è·å– Skill çš„å­—æ®µå®šä¹‰
    registry = get_registry()
    skill = registry.get(session.skill_id)

    fields = []
    if skill:
        if session.requirements:
            normalized_requirements = _normalize_extracted_fields(session.requirements, skill)
            if normalized_requirements != session.requirements:
                session.requirements = normalized_requirements
                workflow.save_session(session)

        fields = [
            {
                "id": f.id,
                "name": f.name,
                "description": f.description,
                "type": f.field_type,
                "required": f.required,
                "collection": f.collection,
                "priority": f.priority,
                "example": f.example,
                "placeholder": f.placeholder,
            }
            for f in skill.requirement_fields
        ]
        if session.skill_overlay and session.skill_overlay.get("relax_requirements"):
            for field in fields:
                field["required"] = False
                if field.get("collection") == "required":
                    field["collection"] = "optional"

    return {
        "session_id": session_id,
        "phase": session.phase,
        "is_complete": session.phase == "complete",
        "has_document": bool(session.final_document),
        "requirements": session.requirements or {},
        "fields": fields,
        "external_information": session.external_information,
        "skill_overlay": session.skill_overlay,
        "planner_plan": session.planner_plan,
        "diagrams": [
            {
                "id": d.get("id"),
                "title": d.get("title"),
                "diagram_type": d.get("diagram_type"),
                "mode": d.get("mode") or ("infographic" if d.get("svg") else "unknown"),
                "created_at": d.get("created_at"),
                "has_png": (flags := _diagram_asset_flags(session_id, d))[0],
                "has_svg": flags[1],
                "png_url": f"/api/chat/session/{session_id}/diagrams/{d.get('id')}.png" if flags[0] else None,
                "svg_url": f"/api/chat/session/{session_id}/diagrams/{d.get('id')}.svg" if flags[1] else None,
            }
            for d in (session.diagrams or [])
            if isinstance(d, dict)
        ],
    }


@router.get("/session/{session_id}/plan")
async def get_planner_plan(session_id: str):
    """è·å–ä¼šè¯çš„ Planner è“å›¾"""
    workflow = get_workflow()
    session = workflow.get_session(session_id)
    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")
    if not session.planner_plan:
        raise HTTPException(status_code=404, detail="Planner è“å›¾å°šæœªç”Ÿæˆ")
    return {"session_id": session_id, "planner_plan": session.planner_plan}


@router.post("/session/{session_id}/start-generation")
async def start_generation(session_id: str):
    """
    å¼€å§‹æ–‡æ¡£ç”Ÿæˆ

    - æ£€æŸ¥å¿…å¡«å­—æ®µæ˜¯å¦å·²å¡«å†™
    - å°†é˜¶æ®µåˆ‡æ¢åˆ° writing
    """
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    # è·å– Skill çš„å­—æ®µå®šä¹‰
    registry = get_registry()
    skill = registry.get(session.skill_id)

    if not skill:
        raise HTTPException(status_code=404, detail=f"Skill not found: {session.skill_id}")

    _ensure_llm_configured()

    # æ£€æŸ¥å¿…å¡«å­—æ®µ
    missing_fields = []
    requirements = session.requirements or {}

    if not (session.skill_overlay and session.skill_overlay.get("relax_requirements")):
        for field in skill.requirement_fields:
            if field.required:
                value = requirements.get(field.id)
                if not value or (isinstance(value, str) and not value.strip()):
                    missing_fields.append(field.name)

    if missing_fields:
        # å‰ç«¯å·²å»é™¤â€œå¿…å¡«å­—æ®µâ€è¡¨å•å±•ç¤ºï¼Œå› æ­¤è¿™é‡Œæ”¹ä¸ºâ€œè½¯æ ¡éªŒâ€ï¼š
        # - æœ‰ææ–™ï¼šå…è®¸ç»§ç»­ç”Ÿæˆï¼Œåç»­åœ¨å†™ä½œè¿‡ç¨‹ä¸­è®©æ¨¡å‹åŸºäºææ–™è¡¥å…¨/è‡ªæ´½
        # - æ— ææ–™ï¼šä»æç¤ºç”¨æˆ·è¡¥å……å…³é”®ä¿¡æ¯
        if session.uploaded_files:
            session.phase = "writing"
            workflow.save_session(session)
            return {
                "success": True,
                "session_id": session_id,
                "phase": "writing",
                "message": "å°†åŸºäºå·²ä¸Šä¼ ææ–™ç»§ç»­ç”Ÿæˆï¼ˆéƒ¨åˆ†å­—æ®µç¼ºå¤±å°†è‡ªåŠ¨è¡¥å…¨/å¼±åŒ–ï¼‰ã€‚",
                "missing_fields": missing_fields,
                "warning": f"ä»¥ä¸‹å­—æ®µåœ¨ææ–™ä¸­æœªæ˜ç¡®æå–åˆ°ï¼š{', '.join(missing_fields)}",
            }
        return {
            "success": False,
            "session_id": session_id,
            "message": f"è¯·å¡«å†™ä»¥ä¸‹å¿…å¡«å­—æ®µ: {', '.join(missing_fields)}",
            "missing_fields": missing_fields,
        }

    # åˆ‡æ¢åˆ°å†™ä½œé˜¶æ®µ
    session.phase = "writing"
    workflow.save_session(session)

    return {
        "success": True,
        "session_id": session_id,
        "phase": "writing",
        "message": "å¼€å§‹ç”Ÿæˆæ–‡æ¡£...",
    }
