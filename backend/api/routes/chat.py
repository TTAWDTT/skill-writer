"""
Chat API è·¯ç”±
å¤„ç†ä¸å·¥ä½œæµçš„äº¤äº’å¯¹è¯ï¼Œæ”¯æŒæµå¼è¾“å‡º
"""
from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional, List, Any, Dict
from pathlib import Path
import re
import json

from backend.core.workflow import get_workflow
from backend.core.skills.registry import get_registry
from backend.core.agents.file_extractor import (
    parse_uploaded_file,
    extract_info_from_multiple_files,
    generate_field_from_files,
)
from backend.core.agents.skill_fixer_agent import SkillFixerAgent
from backend.core.llm.config_store import has_llm_credentials

try:
    import multipart  # noqa: F401
    MULTIPART_AVAILABLE = True
except Exception:
    MULTIPART_AVAILABLE = False

router = APIRouter()


def _ensure_llm_configured():
    if not has_llm_credentials():
        raise HTTPException(status_code=400, detail="æ¨¡å‹æœªé…ç½®")


class StartSessionRequest(BaseModel):
    """å¼€å§‹ä¼šè¯è¯·æ±‚"""
    skill_id: str


class ChatRequest(BaseModel):
    """å¯¹è¯è¯·æ±‚"""
    session_id: str
    message: str


class SessionResponse(BaseModel):
    """ä¼šè¯å“åº”"""
    session_id: str
    phase: str
    message: str
    is_complete: bool
    document: Optional[str] = None


class UploadFilePayload(BaseModel):
    """JSON ä¸Šä¼ æ–‡ä»¶"""
    filename: str
    content_base64: str
    content_type: Optional[str] = None


class UploadFilesRequest(BaseModel):
    """JSON ä¸Šä¼ è¯·æ±‚"""
    files: List[UploadFilePayload]


class GenerateFieldRequest(BaseModel):
    """ç”Ÿæˆå•ä¸ªå­—æ®µè¯·æ±‚"""
    field_id: str


@router.post("/start", response_model=SessionResponse)
async def start_session(request: StartSessionRequest):
    """
    å¼€å§‹æ–°ä¼šè¯

    - ä¼ å…¥ skill_idï¼Œåˆ›å»ºæ–°ä¼šè¯
    - è¿”å›åˆå§‹é—®å€™è¯­å’Œ session_id
    """
    # éªŒè¯ skill å­˜åœ¨
    registry = get_registry()
    skill = registry.get(request.skill_id)
    if not skill:
        raise HTTPException(status_code=404, detail=f"Skill not found: {request.skill_id}")

    _ensure_llm_configured()

    # å¼€å§‹ä¼šè¯
    workflow = get_workflow()
    result = await workflow.start_session(request.skill_id)

    if "error" in result:
        raise HTTPException(status_code=500, detail=result["error"])

    return SessionResponse(
        session_id=result["session_id"],
        phase=result["phase"],
        message=result["message"],
        is_complete=result["is_complete"],
    )


@router.post("/message", response_model=SessionResponse)
async def send_message(request: ChatRequest):
    """
    å‘é€æ¶ˆæ¯

    - åœ¨éœ€æ±‚æ”¶é›†é˜¶æ®µï¼Œå‘é€ç”¨æˆ·å›å¤
    - å¦‚æœéœ€æ±‚æ”¶é›†å®Œæˆï¼Œè‡ªåŠ¨è¿›å…¥å†™ä½œé˜¶æ®µ
    """
    _ensure_llm_configured()
    workflow = get_workflow()
    result = await workflow.chat(request.session_id, request.message)

    if "error" in result:
        raise HTTPException(status_code=400, detail=result["error"])

    return SessionResponse(
        session_id=result["session_id"],
        phase=result["phase"],
        message=result["message"],
        is_complete=result.get("is_complete", False),
        document=result.get("document"),
    )


@router.post("/generate/{session_id}")
async def generate_document(session_id: str):
    """
    ç”Ÿæˆæ–‡æ¡£ï¼ˆéæµå¼ï¼‰

    - åœ¨ writing é˜¶æ®µè°ƒç”¨
    - è¿”å›ç”Ÿæˆçš„å®Œæ•´æ–‡æ¡£
    """
    _ensure_llm_configured()
    workflow = get_workflow()
    result = await workflow.generate_document(session_id)

    if "error" in result:
        raise HTTPException(status_code=400, detail=result["error"])

    return SessionResponse(
        session_id=result["session_id"],
        phase=result["phase"],
        message=result["message"],
        is_complete=result.get("is_complete", False),
        document=result.get("document"),
    )


@router.get("/generate/{session_id}/stream")
async def generate_document_stream(session_id: str):
    """
    æµå¼ç”Ÿæˆæ–‡æ¡£ï¼ˆSSEï¼‰

    - åœ¨ writing é˜¶æ®µè°ƒç”¨
    - å®æ—¶è¿”å›ç”Ÿæˆè¿‡ç¨‹
    """
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    if session.phase != "writing":
        raise HTTPException(
            status_code=400,
            detail=f"Session not in writing phase: {session.phase}"
        )

    _ensure_llm_configured()

    async def event_generator():
        try:
            async for event in workflow.generate_document_stream(session_id):
                # æ ¼å¼åŒ–ä¸º SSE
                data = json.dumps(event, ensure_ascii=False)
                yield f"data: {data}\n\n"
        except Exception as e:
            error_event = json.dumps({"type": "error", "error": str(e)})
            yield f"data: {error_event}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


@router.get("/session/{session_id}")
async def get_session(session_id: str):
    """è·å–ä¼šè¯çŠ¶æ€"""
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    return {
        "session_id": session.session_id,
        "skill_id": session.skill_id,
        "phase": session.phase,
        "has_document": session.final_document is not None,
        "created_at": session.created_at,
        "updated_at": session.updated_at,
        "message_count": len(session.messages),
    }


@router.get("/session/{session_id}/messages")
async def get_session_messages(session_id: str):
    """è·å–ä¼šè¯æ¶ˆæ¯å†å²"""
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    return {
        "session_id": session.session_id,
        "messages": session.messages,
    }


@router.get("/session/{session_id}/document")
async def get_session_document(session_id: str):
    """è·å–ä¼šè¯ç”Ÿæˆçš„æ–‡æ¡£"""
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    if not session.final_document:
        raise HTTPException(status_code=404, detail="Document not generated yet")

    return {
        "session_id": session.session_id,
        "document": session.final_document,
        "sections": session.sections,
    }

def _build_skill_fields(skill) -> List[dict]:
    target_fields = list(skill.requirement_fields)

    collection_rank = {"required": 0, "infer": 1, "optional": 2}
    target_fields.sort(
        key=lambda f: (
            collection_rank.get(f.collection, 2),
            f.priority,
            f.name,
        )
    )
    return [
        {
            "id": f.id,
            "name": f.name,
            "description": f.description,
            "type": f.field_type,
            "required": f.required,
            "collection": f.collection,
            "priority": f.priority,
            "example": f.example,
        }
        for f in target_fields
    ]


def _try_parse_json_value(value: Any):
    if not isinstance(value, str):
        return value
    stripped = value.strip()
    if not stripped:
        return value

    candidates = []
    if (stripped.startswith("{") and stripped.endswith("}")) or (stripped.startswith("[") and stripped.endswith("]")):
        candidates.append(stripped)

    fence_matches = re.findall(r"```(?:json)?\s*([\s\S]*?)\s*```", stripped)
    candidates.extend(fence_matches)

    for pattern in (r"\{[\s\S]*?\}", r"\[[\s\S]*?\]"):
        match = re.search(pattern, stripped)
        if match:
            candidates.append(match.group())

    for candidate in candidates:
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            continue

    return value


def _flatten_list_value(values: list, field_type: str, field_id: str) -> str:
    separator = "\n" if field_type == "textarea" else "ã€"
    parts = []
    for item in values:
        normalized = _normalize_extracted_value(item, field_type, "", field_id)
        if normalized is None:
            continue
        parts.append(str(normalized))
    return separator.join(parts).strip()


def _normalize_key(text: str) -> str:
    return re.sub(r"[^0-9a-zA-Z\u4e00-\u9fff]+", "", str(text)).lower()


def _find_partial_key(tokens: list, key_lookup_normalized: dict) -> Optional[str]:
    for token in tokens:
        normalized_token = _normalize_key(token)
        if not normalized_token:
            continue
        for normalized_key, original in key_lookup_normalized.items():
            if normalized_token in normalized_key:
                return original
    return None


def _extract_value_from_unparsed_json(text: str, keys: list) -> Optional[str]:
    if not isinstance(text, str):
        return None
    if not keys:
        return None

    for key in keys:
        if not key:
            continue
        escaped = re.escape(str(key))
        patterns = [
            rf'"{escaped}"\s*:\s*"([\s\S]*?)"',
            rf"'{escaped}'\s*:\s*'([\s\S]*?)'",
            rf'â€œ{escaped}â€\s*:\s*â€œ([\s\S]*?)â€',
        ]
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()

    return None


def _flatten_dict_value(value_map: dict, field_type: str, field_name: str, field_id: str) -> str:
    if not value_map:
        return ""

    name_hint = (field_name or "").lower()
    prefer_title = field_type != "textarea" and any(k in name_hint for k in ["name", "title", "åç§°", "æ ‡é¢˜", "é¢˜ç›®"])
    content_keys = ["content", "æ­£æ–‡", "å†…å®¹", "text", "body", "detail", "details", "description", "summary", "ç®€ä»‹", "è¯´æ˜", "èƒŒæ™¯"]
    title_keys = ["title", "æ ‡é¢˜", "name", "åç§°", "topic", "subject", "é¡¹ç›®åç§°", "è¯¾é¢˜åç§°"]

    key_order = title_keys + content_keys if prefer_title else content_keys + title_keys
    key_lookup = {str(k).lower(): k for k in value_map.keys()}
    key_lookup_normalized = {_normalize_key(k): k for k in value_map.keys()}
    field_id_key = _normalize_key(field_id)
    field_name_key = _normalize_key(field_name)

    if field_id_key in key_lookup_normalized:
        raw_value = value_map.get(key_lookup_normalized[field_id_key])
        normalized = _normalize_extracted_value(raw_value, field_type, field_name, field_id)
        return str(normalized).strip() if normalized is not None else ""

    if field_name_key and field_name_key in key_lookup_normalized:
        raw_value = value_map.get(key_lookup_normalized[field_name_key])
        normalized = _normalize_extracted_value(raw_value, field_type, field_name, field_id)
        return str(normalized).strip() if normalized is not None else ""

    partial_key = _find_partial_key([field_id, field_name], key_lookup_normalized)
    if partial_key:
        raw_value = value_map.get(partial_key)
        normalized = _normalize_extracted_value(raw_value, field_type, field_name, field_id)
        return str(normalized).strip() if normalized is not None else ""

    for key in key_order:
        if key in key_lookup:
            raw_value = value_map.get(key_lookup[key])
            normalized = _normalize_extracted_value(raw_value, field_type, field_name, field_id)
            if normalized is None:
                continue
            return str(normalized).strip()

    partial_key = _find_partial_key(key_order, key_lookup_normalized)
    if partial_key:
        raw_value = value_map.get(partial_key)
        normalized = _normalize_extracted_value(raw_value, field_type, field_name, field_id)
        return str(normalized).strip() if normalized is not None else ""

    if len(value_map) == 1:
        only_value = next(iter(value_map.values()))
        normalized = _normalize_extracted_value(only_value, field_type, field_name, field_id)
        return str(normalized).strip() if normalized is not None else ""

    separator = "\n" if field_type == "textarea" else "ï¼Œ"
    parts = []
    for key, raw_value in value_map.items():
        normalized = _normalize_extracted_value(raw_value, field_type, field_name, field_id)
        if normalized is None:
            continue
        parts.append(f"{key}: {normalized}")
    return separator.join(parts).strip()


def _normalize_extracted_value(value: Any, field_type: str, field_name: str, field_id: str) -> Any:
    if value is None:
        return None

    parsed = _try_parse_json_value(value)

    if isinstance(parsed, dict):
        return _flatten_dict_value(parsed, field_type, field_name, field_id)
    if isinstance(parsed, list):
        return _flatten_list_value(parsed, field_type, field_id)
    if isinstance(parsed, str):
        stripped = parsed.strip()
        if stripped.startswith("{") or stripped.startswith("["):
            title_keys = ["title", "name", "æ ‡é¢˜", "åç§°", "topic", "subject", "é¡¹ç›®åç§°", "è¯¾é¢˜åç§°", "project_title"]
            content_keys = ["content", "æ­£æ–‡", "å†…å®¹", "text", "body", "detail", "details", "description", "summary", "ç®€ä»‹", "è¯´æ˜", "èƒŒæ™¯"]
            key_candidates = [field_id, field_name] + title_keys + content_keys
            extracted = _extract_value_from_unparsed_json(stripped, key_candidates)
            if extracted:
                return extracted

    return parsed


def _normalize_extracted_fields(extracted_fields: Dict[str, Any], skill) -> Dict[str, Any]:
    if not extracted_fields:
        return {}

    field_map = {f.id: f for f in skill.requirement_fields}
    normalized = {}
    for field_id, value in extracted_fields.items():
        field = field_map.get(field_id)
        field_type = field.field_type if field else "text"
        field_name = field.name if field else field_id
        normalized_value = _normalize_extracted_value(value, field_type, field_name, field_id)
        if normalized_value is None:
            continue
        if isinstance(normalized_value, str) and not normalized_value.strip():
            continue
        normalized[field_id] = normalized_value
    return normalized


def _trim_file_content(content: str, max_chars: int = 20000) -> str:
    """é™åˆ¶å­˜å‚¨çš„æ–‡ä»¶å†…å®¹é•¿åº¦ï¼Œé¿å…æ•°æ®åº“è¿‡å¤§"""
    if not content:
        return ""
    content = content.strip()
    if len(content) <= max_chars:
        return content
    return content[:max_chars]


async def _handle_parsed_upload(
    session,
    skill,
    workflow,
    parsed_files: List[dict],
    file_summaries: List[str],
):
    _ensure_llm_configured()
    if not parsed_files:
        return {
            "success": False,
            "session_id": session.session_id,
            "message": "æ²¡æœ‰æˆåŠŸè§£æä»»ä½•æ–‡ä»¶",
            "file_results": file_summaries,
            "extracted_fields": {},
            "external_information": "",
        }

    skill_fields = _build_skill_fields(skill)
    extraction_result = {
        "extracted_fields": {},
        "external_information": "",
        "summaries": "",
    }

    try:
        extraction_result = await extract_info_from_multiple_files(
            files=parsed_files,
            skill_fields=skill_fields,
            skill_name=skill.metadata.name,
            existing_requirements=session.requirements,
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¿¡æ¯æå–å¤±è´¥: {str(e)}") from e

    extraction_result["extracted_fields"] = _normalize_extracted_fields(
        extraction_result.get("extracted_fields", {}),
        skill,
    )

    # æ›´æ–°ä¼šè¯çŠ¶æ€
    for pf in parsed_files:
        session.add_uploaded_file({
            "filename": pf["filename"],
            "content_type": pf.get("content_type", ""),
            "size": pf.get("size", 0),
            "content": _trim_file_content(pf.get("content", "")),
            "extracted_fields": extraction_result.get("extracted_fields", {}),
        })

    # è¿½åŠ å¤–éƒ¨ä¿¡æ¯
    external_info = extraction_result.get("external_information", "")
    if external_info:
        session.append_external_info(external_info)

    # åŸºäºä¸Šä¼ ææ–™ä¿®è¡¥ Skillï¼ˆä»…å½“å‰ä¼šè¯ï¼‰
    try:
        fixer = SkillFixerAgent()
        fixer_result = await fixer.run(
            skill=skill,
            extracted_fields=extraction_result.get("extracted_fields", {}),
            external_information=session.external_information,
            file_summaries=extraction_result.get("summaries", ""),
        )
        session.skill_overlay = {
            "writing_guidelines_additions": fixer_result.writing_guidelines_additions,
            "global_principles": fixer_result.global_principles,
            "section_overrides": fixer_result.section_overrides,
            "relax_requirements": fixer_result.relax_requirements,
            "material_context": fixer_result.material_context,
            "section_prompt_overrides": fixer_result.section_prompt_overrides,
        }
    except Exception as e:
        print(f"[Skill Fixer Warning] {e}")

    # å°†æå–çš„å­—æ®µåˆå¹¶åˆ°éœ€æ±‚ä¸­
    extracted_fields = extraction_result.get("extracted_fields", {})
    if extracted_fields:
        if session.requirements is None:
            session.requirements = {}
        for field_id, value in extracted_fields.items():
            if value is None:
                continue
            if isinstance(value, str) and not value.strip():
                continue
            existing_value = session.requirements.get(field_id)
            if existing_value is None or (isinstance(existing_value, str) and not existing_value.strip()):
                session.requirements[field_id] = value

    # ä¿å­˜ä¼šè¯
    workflow.save_session(session)

    # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯åˆ°å¯¹è¯å†å²
    upload_message = f"ğŸ“ å·²ä¸Šä¼  {len(parsed_files)} ä¸ªæ–‡ä»¶å¹¶æå–ä¿¡æ¯ï¼š\n" + "\n".join(file_summaries)
    session.messages.append({
        "role": "system",
        "content": upload_message,
    })
    workflow.save_session(session)

    return {
        "success": True,
        "session_id": session.session_id,
        "message": f"æˆåŠŸå¤„ç† {len(parsed_files)} ä¸ªæ–‡ä»¶",
        "file_results": file_summaries,
        "extracted_fields": extracted_fields,
        "external_information": external_info[:500] + "..." if len(external_info) > 500 else external_info,
        "summaries": extraction_result.get("summaries", ""),
    }


@router.post("/session/{session_id}/upload-json")
async def upload_files_json(
    session_id: str,
    payload: UploadFilesRequest,
):
    """
    ä¸Šä¼ æ–‡ä»¶åˆ°ä¼šè¯

    - æ”¯æŒä¸Šä¼ å¤šä¸ªæ–‡ä»¶
    - è‡ªåŠ¨è§£ææ–‡ä»¶å†…å®¹å¹¶ä½¿ç”¨ LLM æå–ç›¸å…³ä¿¡æ¯
    - è¿”å›æå–çš„ä¿¡æ¯æ‘˜è¦
    """
    # éªŒè¯ä¼šè¯å­˜åœ¨
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    if session.phase not in ["init", "requirement"]:
        raise HTTPException(
            status_code=400,
            detail=f"Cannot upload files in phase: {session.phase}. Only allowed during requirement collection."
        )

    # è·å– Skill ä¿¡æ¯
    registry = get_registry()
    skill = registry.get(session.skill_id)

    if not skill:
        raise HTTPException(status_code=404, detail=f"Skill not found: {session.skill_id}")

    _ensure_llm_configured()

    # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
    allowed_extensions = {'.md', '.txt', '.doc', '.docx', '.pdf', '.pptx'}

    # è§£ææ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶
    parsed_files = []
    file_summaries = []

    for file in payload.files:
        file_ext = Path(file.filename).suffix.lower()

        if file_ext not in allowed_extensions:
            file_summaries.append(f"âŒ {file.filename}: ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ ({file_ext})")
            continue

        try:
            import base64
            content = base64.b64decode(file.content_base64)
            text_content = parse_uploaded_file(content, file_ext, file.filename)

            if text_content:
                parsed_files.append({
                    "filename": file.filename,
                    "content": text_content,
                    "content_type": file.content_type or "",
                    "size": len(content),
                })
                file_summaries.append(f"âœ… {file.filename}: è§£ææˆåŠŸ ({len(text_content)} å­—ç¬¦)")
            else:
                file_summaries.append(f"âš ï¸ {file.filename}: æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è§£æ")

        except Exception as e:
            file_summaries.append(f"âŒ {file.filename}: è§£æå¤±è´¥ - {str(e)}")

    try:
        return await _handle_parsed_upload(session, skill, workflow, parsed_files, file_summaries)
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        print(f"[File Upload Error] {traceback.format_exc()}")
        raise HTTPException(
            status_code=500,
            detail=str(e)
        )


if MULTIPART_AVAILABLE:
    @router.post("/session/{session_id}/upload")
    async def upload_files(
        session_id: str,
        files: List[UploadFile] = File(...),
    ):
        """
        ä¸Šä¼ æ–‡ä»¶åˆ°ä¼šè¯

        - æ”¯æŒä¸Šä¼ å¤šä¸ªæ–‡ä»¶
        - è‡ªåŠ¨è§£ææ–‡ä»¶å†…å®¹å¹¶ä½¿ç”¨ LLM æå–ç›¸å…³ä¿¡æ¯
        - è¿”å›æå–çš„ä¿¡æ¯æ‘˜è¦
        """
        # éªŒè¯ä¼šè¯å­˜åœ¨
        workflow = get_workflow()
        session = workflow.get_session(session_id)

        if not session:
            raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

        if session.phase not in ["init", "requirement"]:
            raise HTTPException(
                status_code=400,
                detail=f"Cannot upload files in phase: {session.phase}. Only allowed during requirement collection."
            )

        # è·å– Skill ä¿¡æ¯
        registry = get_registry()
        skill = registry.get(session.skill_id)

        if not skill:
            raise HTTPException(status_code=404, detail=f"Skill not found: {session.skill_id}")

        _ensure_llm_configured()

        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        allowed_extensions = {'.md', '.txt', '.doc', '.docx', '.pdf', '.pptx'}

        # è§£ææ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶
        parsed_files = []
        file_summaries = []

        for file in files:
            file_ext = Path(file.filename).suffix.lower()

            if file_ext not in allowed_extensions:
                file_summaries.append(f"âŒ {file.filename}: ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ ({file_ext})")
                continue

            try:
                content = await file.read()
                text_content = parse_uploaded_file(content, file_ext, file.filename)

                if text_content:
                    parsed_files.append({
                        "filename": file.filename,
                        "content": text_content,
                        "content_type": file.content_type,
                        "size": len(content),
                    })
                    file_summaries.append(f"âœ… {file.filename}: è§£ææˆåŠŸ ({len(text_content)} å­—ç¬¦)")
                else:
                    file_summaries.append(f"âš ï¸ {file.filename}: æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è§£æ")

            except Exception as e:
                file_summaries.append(f"âŒ {file.filename}: è§£æå¤±è´¥ - {str(e)}")

        try:
            return await _handle_parsed_upload(session, skill, workflow, parsed_files, file_summaries)
        except HTTPException:
            raise
        except Exception as e:
            import traceback
            print(f"[File Upload Error] {traceback.format_exc()}")
            raise HTTPException(
                status_code=500,
                detail=str(e)
            )


@router.get("/session/{session_id}/files")
async def get_session_files(session_id: str):
    """è·å–ä¼šè¯ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨"""
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    files = [
        {k: v for k, v in file_info.items() if k != "content"}
        for file_info in session.uploaded_files
    ]

    return {
        "session_id": session.session_id,
        "files": files,
        "external_information": session.external_information,
    }


@router.post("/session/{session_id}/generate-field")
async def generate_field(session_id: str, request: GenerateFieldRequest):
    """åŸºäºå·²ä¸Šä¼ ææ–™ç”Ÿæˆå•ä¸ªå­—æ®µå†…å®¹"""
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    if not session.uploaded_files:
        raise HTTPException(status_code=400, detail="No uploaded files found for this session")

    if not has_llm_credentials():
        raise HTTPException(status_code=400, detail="æ¨¡å‹æœªé…ç½®")

    registry = get_registry()
    skill = registry.get(session.skill_id)
    if not skill:
        raise HTTPException(status_code=404, detail=f"Skill not found: {session.skill_id}")

    field = next((f for f in skill.requirement_fields if f.id == request.field_id), None)
    if not field:
        raise HTTPException(status_code=404, detail=f"Field not found: {request.field_id}")

    files = [
        {
            "filename": f.get("filename", "unknown"),
            "content": f.get("content", ""),
        }
        for f in session.uploaded_files
    ]

    if not any(f.get("content") for f in files):
        raise HTTPException(status_code=400, detail="No file content available; please re-upload files")

    result = await generate_field_from_files(
        files=files,
        field={
            "id": field.id,
            "name": field.name,
            "description": field.description,
            "type": field.field_type,
        },
        skill_name=skill.metadata.name,
        existing_requirements=session.requirements,
        external_information=session.external_information,
    )

    value = result.get("value")
    value = _normalize_extracted_value(value, field.field_type, field.name, field.id)
    if value is None or (isinstance(value, str) and not value.strip()):
        return {
            "success": False,
            "session_id": session_id,
            "field_id": field.id,
            "message": "æœªåœ¨ææ–™ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯",
            "value": None,
        }

    if session.requirements is None:
        session.requirements = {}
    session.requirements[field.id] = value
    workflow.save_session(session)

    return {
        "success": True,
        "session_id": session_id,
        "field_id": field.id,
        "value": value,
    }


class UpdateRequirementsRequest(BaseModel):
    """æ›´æ–°éœ€æ±‚è¯·æ±‚"""
    requirements: dict


@router.put("/session/{session_id}/requirements")
async def update_requirements(session_id: str, request: UpdateRequirementsRequest):
    """
    ç›´æ¥æ›´æ–°ä¼šè¯çš„éœ€æ±‚å­—æ®µ

    - ç”¨äºè¡¨å•ç›´æ¥ç¼–è¾‘éœ€æ±‚
    - ä¸éœ€è¦é€šè¿‡å¯¹è¯æ”¶é›†
    """
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    # æ›´æ–°éœ€æ±‚
    if session.requirements is None:
        session.requirements = {}

    # åˆå¹¶æ–°çš„éœ€æ±‚ï¼ˆä¿ç•™å·²æœ‰å€¼ï¼Œé™¤éæ˜ç¡®è¦†ç›–ï¼‰
    for key, value in request.requirements.items():
        if value is None:
            session.requirements.pop(key, None)
            continue
        if isinstance(value, str) and not value.strip():
            session.requirements.pop(key, None)
            continue
        session.requirements[key] = value

    # ä¿å­˜ä¼šè¯
    workflow.save_session(session)

    return {
        "success": True,
        "session_id": session_id,
        "requirements": session.requirements,
    }


@router.get("/session/{session_id}/requirements")
async def get_requirements(session_id: str):
    """è·å–ä¼šè¯çš„éœ€æ±‚å­—æ®µ"""
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    # è·å– Skill çš„å­—æ®µå®šä¹‰
    registry = get_registry()
    skill = registry.get(session.skill_id)

    fields = []
    if skill:
        if session.requirements:
            normalized_requirements = _normalize_extracted_fields(session.requirements, skill)
            if normalized_requirements != session.requirements:
                session.requirements = normalized_requirements
                workflow.save_session(session)

        fields = [
            {
                "id": f.id,
                "name": f.name,
                "description": f.description,
                "type": f.field_type,
                "required": f.required,
                "collection": f.collection,
                "priority": f.priority,
                "example": f.example,
                "placeholder": f.placeholder,
            }
            for f in skill.requirement_fields
        ]
        if session.skill_overlay and session.skill_overlay.get("relax_requirements"):
            for field in fields:
                field["required"] = False
                if field.get("collection") == "required":
                    field["collection"] = "optional"

    return {
        "session_id": session_id,
        "requirements": session.requirements or {},
        "fields": fields,
        "external_information": session.external_information,
        "skill_overlay": session.skill_overlay,
    }


@router.post("/session/{session_id}/start-generation")
async def start_generation(session_id: str):
    """
    å¼€å§‹æ–‡æ¡£ç”Ÿæˆ

    - æ£€æŸ¥å¿…å¡«å­—æ®µæ˜¯å¦å·²å¡«å†™
    - å°†é˜¶æ®µåˆ‡æ¢åˆ° writing
    """
    workflow = get_workflow()
    session = workflow.get_session(session_id)

    if not session:
        raise HTTPException(status_code=404, detail=f"Session not found: {session_id}")

    # è·å– Skill çš„å­—æ®µå®šä¹‰
    registry = get_registry()
    skill = registry.get(session.skill_id)

    if not skill:
        raise HTTPException(status_code=404, detail=f"Skill not found: {session.skill_id}")

    _ensure_llm_configured()

    # æ£€æŸ¥å¿…å¡«å­—æ®µ
    missing_fields = []
    requirements = session.requirements or {}

    if not (session.skill_overlay and session.skill_overlay.get("relax_requirements")):
        for field in skill.requirement_fields:
            if field.required:
                value = requirements.get(field.id)
                if not value or (isinstance(value, str) and not value.strip()):
                    missing_fields.append(field.name)

    if missing_fields:
        return {
            "success": False,
            "session_id": session_id,
            "message": f"è¯·å¡«å†™ä»¥ä¸‹å¿…å¡«å­—æ®µ: {', '.join(missing_fields)}",
            "missing_fields": missing_fields,
        }

    # åˆ‡æ¢åˆ°å†™ä½œé˜¶æ®µ
    session.phase = "writing"
    workflow.save_session(session)

    return {
        "success": True,
        "session_id": session_id,
        "phase": "writing",
        "message": "å¼€å§‹ç”Ÿæˆæ–‡æ¡£...",
    }
